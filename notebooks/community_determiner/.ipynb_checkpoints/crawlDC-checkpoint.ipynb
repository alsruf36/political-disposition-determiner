{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e236926-9f1b-4dd3-a706-79581a882cf5",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "9bd58864-155c-49f7-9abf-fda0cb006655",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#git clone -b prototype --single-branch https://github.com/alsruf36/political-disposition-determiner.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567558aa-a17f-444c-a0c4-2abc4dfd176b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "e09ccdb2-92f2-4353-b880-778da0c3716c",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install dill\n",
    "!pip install ray\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60d628-87d6-40b6-b519-cc4786787f76",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "455bc4ff-d374-4a11-b6f8-cdd8b532e8bf",
     "isComponent": true,
     "name": "import_packages",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\n",
    "import time\n",
    "import re\n",
    "import dill as pickle\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import tqdm\n",
    "from pprint import pprint\n",
    "import ray\n",
    "from pymongo import MongoClient\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26111d-182b-4a3c-a2f2-2e8cac03ec85",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "0d487371-c4fc-44ed-a77e-87b5b293556f",
     "isComponent": true,
     "name": "define_driver",
     "parents": [
      {
       "id": "455bc4ff-d374-4a11-b6f8-cdd8b532e8bf",
       "name": "import_packages"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "def get_selenium_driver(ip):\n",
    "    capabilities = {\n",
    "        \"browserName\": \"chrome\",\n",
    "        \"browserVersion\": \"latest\",\n",
    "        \"selenoid:options\": {\n",
    "            \"enableVNC\": True,\n",
    "            \"enableVideo\": False\n",
    "        }\n",
    "    }\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36')\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    #options.add_argument(\"--proxy-server=socks5://10.26.0.189:9050\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    driver = webdriver.Remote(\n",
    "        command_executor=f\"http://{ip}/callisto\",\n",
    "        options=options,\n",
    "        desired_capabilities=capabilities)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a0f77-57fb-420e-843f-5f9f487f6c14",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "b152beca-de2a-42ba-a62c-b4e34f932f1c",
     "isComponent": true,
     "name": "define_crawl_class",
     "parents": [
      {
       "id": "0d487371-c4fc-44ed-a77e-87b5b293556f",
       "name": "define_driver"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class crawl:\n",
    "    def __init__(self, galleryId):\n",
    "        self.galleryId = galleryId\n",
    "    \n",
    "    def allocate_driver(self, ip):\n",
    "        self.driver = get_selenium_driver(ip)\n",
    "        \n",
    "    def delocate_driver(self):\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "    \n",
    "    def articleList(self, page):\n",
    "        id = self.galleryId\n",
    "        url = f\"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend\"\n",
    "        self.driver.get(url)\n",
    "        self.driver.implicitly_wait(3)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        article_list = self.driver.find_element(By.TAG_NAME, \"tbody\").find_elements(By.TAG_NAME, \"tr\")\n",
    "        return article_list\n",
    "    \n",
    "    def articleLink(self, article_list):\n",
    "        title_link = []\n",
    "        \n",
    "        try:\n",
    "            for item in article_list:\n",
    "\n",
    "                title_item = item.find_element(By.TAG_NAME, \"a\")\n",
    "                title_item = title_item.get_attribute('href')\n",
    "                if(title_item is None or self.galleryId not in title_item):\n",
    "                    continue\n",
    "\n",
    "                title_link.append(title_item)\n",
    "                print(\"new title link added at title_link[\",len(title_link)-1, \"]:\", title_item)\n",
    "        \n",
    "        except:\n",
    "            return title_link\n",
    "    \n",
    "        return title_link\n",
    "                \n",
    "    \n",
    "    def articleText(self, crawledLink, minLen):\n",
    "        text_list = []\n",
    "        num = ['0','1','2','3','4','5','6','7','8','9']\n",
    "        \n",
    "        \n",
    "        for link in crawledLink:\n",
    "            print(\"==============START====================\")\n",
    "            self.driver.get(link)\n",
    "            self.driver.implicitly_wait(10)\n",
    "            time.sleep(2)\n",
    "            sentence = [\"\",\"\"] # 본문, 제목\n",
    "            \n",
    "            article_text = self.driver.find_element(By.CLASS_NAME, \"write_div\")\n",
    "            try:\n",
    "                article_text = article_text.find_elements(By.TAG_NAME, \"p\")\n",
    "                \n",
    "            except:\n",
    "                try:\n",
    "                    article_text = article_text.find_elements(By.TAG_NAME, \"div\")\n",
    "                except:\n",
    "                    article_text = article_text.find_elements(By.TAG_NAME, \"span\")\n",
    "            \n",
    "    \n",
    "            \n",
    "            boolFound = False\n",
    "            \n",
    "            for i in article_text:\n",
    "                text = i.text\n",
    "                \n",
    "                urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', i.text)\n",
    "                if len(urls) > 0:\n",
    "                    print(f'url exist, replaced. the text is :{text}\\n=======')\n",
    "                    for str_link in urls:\n",
    "                        text = text.replace(str_link, \"\")\n",
    "                    boolFound = False\n",
    "                    \n",
    "                if \"이미지 순서 ON\" in text:\n",
    "                    print(f'\"이미지 순서 ON\" exist, replaced. the text is :{text}\\n=======')\n",
    "                    boolFound = True\n",
    "                    continue\n",
    "                \n",
    "                if \"- dc official App\" in text:\n",
    "                    print(f'\"- dc official App\" exist, replaced. the text is :{text}\\n=======')\n",
    "                    boolFound = False\n",
    "                    continue\n",
    "                    \n",
    "                if boolFound is True and ((text >= '0' and text <= '9') or \" \" in text):\n",
    "                    print(f'number after \"이미지 순서 ON\", the text is :{text}\\n=======')\n",
    "                    continue\n",
    "                \n",
    "                if text in sentence:\n",
    "                    print(f'text already exist, text is {text} | sentence is {sentence}\\n=======')\n",
    "                    boolFound = False\n",
    "                    continue\n",
    "                \n",
    "                boolFound = False\n",
    "                text = text.replace(\"\\n\",\" \")\n",
    "                print(f'removed every trash data at text, the text is :{text}\\n=======')\n",
    "                    \n",
    "                sentence[0] += text\n",
    "\n",
    "            if(len(sentence[0]) < minLen):\n",
    "                print(\"article text not added! (len < minLen) | link : \",link)\n",
    "                print(\"the text is :\", sentence[0])\n",
    "                print(\"============END======================\\n\\n\\n\")\n",
    "                continue\n",
    "\n",
    "            sentence[1] = self.driver.find_element(By.CLASS_NAME, \"title_subject\").text\n",
    "            print(\"article text added at text_list[\",len(text_list),\"]\",\"(link :\",link,\") :\")\n",
    "            print(sentence)\n",
    "            text_list.append(sentence)\n",
    "            print(\"============END======================\\n\\n\\n\")\n",
    "            \n",
    "        return text_list\n",
    "    \n",
    "    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n",
    "    # try:\n",
    "        text_list = []\n",
    "        for link in tqdm.tqdm(crawledLink):\n",
    "            try_n = 0\n",
    "            while try_n < 2:\n",
    "                try:\n",
    "                    print(\"==============START====================\")\n",
    "                    self.driver.get(link)\n",
    "                    self.driver.implicitly_wait(10)\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    try:\n",
    "                        articleDateHour = self.driver.find_element(By.CLASS_NAME, \"gall_date\").text.split(\" \")\n",
    "                    except:\n",
    "                        time.sleep(2)\n",
    "                        articleDateHour = self.driver.find_element(By.CLASS_NAME, \"gall_date\").text.split(\" \")\n",
    "\n",
    "                    articleDate = articleDateHour[0].split(\".\")\n",
    "                    articleHour = articleDateHour[1].split(\":\")\n",
    "                    articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[1]),int(articleHour[2]), 0).timestamp()\n",
    "\n",
    "                    if(limUnixDate > articleUnixDate):\n",
    "                        print(\"too old articleText!\")\n",
    "                        print(\"============END======================\\n\\n\\n\")\n",
    "                        raise\n",
    "\n",
    "                    sentence = [\"\",articleUnixDate,\"\"]\n",
    "\n",
    "                    article_text = self.driver.find_element(By.CLASS_NAME, \"write_div\")\n",
    "                    try:\n",
    "                        article_text = article_text.find_elements(By.TAG_NAME, \"div\")\n",
    "                        checkRaise = True\n",
    "                        for i in article_text:\n",
    "                            if i.text == '':\n",
    "                                continue\n",
    "                            else:\n",
    "                                checkRaise = False\n",
    "                                break\n",
    "\n",
    "                        if checkRaise:\n",
    "                            raise\n",
    "\n",
    "                    except:\n",
    "                        try:\n",
    "                            article_text = self.driver.find_element(By.CLASS_NAME, \"write_div\")\n",
    "                            article_text = article_text.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "                        except:\n",
    "                            article_text = article_text.find_elements(By.TAG_NAME, \"span\")\n",
    "\n",
    "\n",
    "\n",
    "                    boolFound = False\n",
    "\n",
    "                    for i in article_text:\n",
    "                        text = i.text\n",
    "\n",
    "                        urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', i.text)\n",
    "                        if len(urls) > 0:\n",
    "                            print(f'url exist, replaced. the text is :{text}\\n=======')\n",
    "                            for str_link in urls:\n",
    "                                text = text.replace(str_link, \"\")\n",
    "                            boolFound = False\n",
    "\n",
    "                        if \"이미지 순서 ON\" in text:\n",
    "                            print(f'\"이미지 순서 ON\" exist, replaced. the text is :{text}\\n=======')\n",
    "                            boolFound = True\n",
    "                            continue\n",
    "\n",
    "                        if \"- dc official App\" in text:\n",
    "                            print(f'\"- dc official App\" exist, replaced. the text is :{text}\\n=======')\n",
    "                            boolFound = False\n",
    "                            continue\n",
    "\n",
    "                        if boolFound is True and ((text >= '0' and text <= '9') or \" \" in text):\n",
    "                            print(f'number after \"이미지 순서 ON\", the text is :{text}\\n=======')\n",
    "                            continue\n",
    "\n",
    "                        if text in sentence:\n",
    "                            print(f'text already exist, text is {text} | sentence is {sentence}\\n=======')\n",
    "                            boolFound = False\n",
    "                            continue\n",
    "\n",
    "                        boolFound = False\n",
    "                        text = text.replace(\"\\n\",\" \")\n",
    "                        print(f'removed every trash data at text, the text is :{text}\\n=======')\n",
    "\n",
    "                        sentence[0] += text\n",
    "\n",
    "                    if(len(sentence[0]) < minLen):\n",
    "                        print(\"article text not added! (len < minLen) | link : \",link)\n",
    "                        print(\"the text is :\", sentence)\n",
    "                        print(\"============END======================\\n\\n\\n\")\n",
    "                        break\n",
    "\n",
    "                    sentence[2] = self.driver.find_element(By.CLASS_NAME, \"title_subject\").text\n",
    "                    print(\"article text added at text_list[\",len(text_list),\"]\",\"(link :\",link,\") :\")\n",
    "                    print(sentence)\n",
    "                    text_list.append(sentence)\n",
    "                    print(\"============END======================\\n\\n\\n\")\n",
    "                    break\n",
    "        \n",
    "                except Exception as e:\n",
    "                    try_n += 1\n",
    "                    print(e)\n",
    "                    print(f\"다시 시도합니다 ... (try_n : {try_n})\")\n",
    "        \n",
    "        return text_list\n",
    "#     except Exception as e:\n",
    "\n",
    "#         print(\"error happend\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd804faf-02c4-487e-9e87-3757c2ca9545",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "d622d1a0-1ca8-43ce-891d-d01b108f6afb",
     "isComponent": true,
     "name": "define_main_function",
     "parents": [
      {
       "id": "b152beca-de2a-42ba-a62c-b4e34f932f1c",
       "name": "define_crawl_class"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_limit_date(year, month, day, hour, minute, second):\n",
    "    limDate = [year, month, day]\n",
    "    limHour = [hour, minute, second]\n",
    "    limUnixDateTime = datetime.datetime(int(limDate[0]), int(limDate[1]), int(limDate[2]), int(limHour[0]), int(limHour[1]),int(limHour[2]), 0).timestamp()\n",
    "    return limUnixDateTime\n",
    "\n",
    "def get_page_content(c, page, limUnixDatetime):\n",
    "    links = c.articleLink(c.articleList(page))\n",
    "    text = c.articleText_withLimit(links,10,limUnixDatetime)\n",
    "    return text\n",
    "\n",
    "def create_document(data, min_len):\n",
    "    title = data[2]\n",
    "    timestamp = data[1]\n",
    "    body = data[0]\n",
    "    \n",
    "    if len(body) < min_len:\n",
    "        return None\n",
    "\n",
    "    sentences = requests.get(\n",
    "                                \"https://api.kshs.dev/analyze\",\n",
    "                                 params={\"sentence\": body, \"clean\": \"true\", \"plural\": \"true\"}\n",
    "                            ).json()\n",
    "    \n",
    "    sentence_id = 0\n",
    "    sentence_arr = []\n",
    "    \n",
    "    cons = 0\n",
    "    pros = 0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence[0]) < 10:\n",
    "            tend = 2\n",
    "            \n",
    "        else:\n",
    "            tend = sentence[1]\n",
    "            if tend == 0:\n",
    "                cons += 1\n",
    "            \n",
    "            elif tend == 1:\n",
    "                pros += 1\n",
    "            \n",
    "        sentence_arr.append({\n",
    "            \"id\": sentence_id,\n",
    "            \"text\": sentence[0],\n",
    "            \"tend\": tend\n",
    "        })\n",
    "        \n",
    "        sentence_id += 1\n",
    "        \n",
    "    if cons == pros:\n",
    "        article_tend = 2\n",
    "    \n",
    "    elif cons > pros:\n",
    "        article_tend = 0\n",
    "        \n",
    "    elif cons < pros:\n",
    "        article_tend = 1\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    uname = str(timestamp)+title\n",
    "    return {\n",
    "        \"_id\": hashlib.sha256(uname.encode()).hexdigest(),\n",
    "        \"title\": title,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"tend\": article_tend,\n",
    "        \"cons\": cons,\n",
    "        \"pros\": pros,\n",
    "        \"body\": sentence_arr\n",
    "    }\n",
    "\n",
    "def insert_document(document, cur):\n",
    "    cur.update_one(\n",
    "        {\"_id\": document['_id']},\n",
    "        {\"$set\": document},\n",
    "        upsert = True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644908d-538b-4b0e-a81b-12105b1c897b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "ca759fa5-682e-4cbe-af92-0f6de312f289",
     "isComponent": true,
     "name": "init_class",
     "parents": [
      {
       "id": "d622d1a0-1ca8-43ce-891d-d01b108f6afb",
       "name": "define_main_function"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gallId = \"newconservativeparty\"\n",
    "limUnixDatetime = get_limit_date(2022, 8, 22, 12, 0, 0)\n",
    "minPage = 3\n",
    "maxPage = 10\n",
    "ip = \"10.99.151.246\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ea4c3-75b6-4b69-bec2-980410706c4c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "b1ca64e4-37d9-42b5-b770-bb07e5e454d4",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "c = crawl(gallId)\n",
    "c.allocate_driver(ip)\n",
    "\n",
    "AuthMongoClient = MongoClient(\"mongodb://@:27010\", authSource=\"admin\")\n",
    "dbclient = AuthMongoClient\n",
    "db = dbclient['community']\n",
    "cur = db['dcinside']\n",
    "\n",
    "crawled_text = []\n",
    "for page in range(minPage, maxPage):\n",
    "    text = get_page_content(c, page, limUnixDatetime)\n",
    "    for data in text:\n",
    "        document = create_document(data, 20)\n",
    "        if not document == None:\n",
    "            insert_document(document, cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f1a7a-9fcf-419a-ab94-cefe5f3f239a",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "f40fb503-cc11-42db-8d8e-90e2f830aa86",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "c.delocate_driver()"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

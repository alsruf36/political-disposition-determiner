[2022-10-25 18:21:48] mrx-link.MRXLinkMagics.mrxlink_set_parameters() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='[]\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666720390.36.1.1666720653.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666720390.36.1.1666720653.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev', no_reply=True)
[2022-10-25 18:22:49] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='{"nodes":[{"id":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","name":"import_packages","code":"from selenium import webdriver\\nfrom selenium.webdriver.chrome.options import Options\\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\\nimport time\\nimport re\\nimport dill as pickle\\nimport datetime\\nimport requests\\nimport json\\nimport tqdm\\nfrom pprint import pprint\\nimport ray\\nfrom pymongo import MongoClient\\nimport hashlib","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"0d487371-c4fc-44ed-a77e-87b5b293556f","name":"define_driver","code":"def get_selenium_driver(ip):\\n    capabilities = {\\n        \\"browserName\\": \\"chrome\\",\\n        \\"browserVersion\\": \\"latest\\",\\n        \\"selenoid:options\\": {\\n            \\"enableVNC\\": True,\\n            \\"enableVideo\\": False\\n        }\\n    }\\n\\n    options = Options()\\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\\n    options.add_experimental_option(\\"excludeSwitches\\", [\\"enable-automation\\"])\\n    options.add_experimental_option(\'useAutomationExtension\', False)\\n    #options.add_argument(\\"--proxy-server=socks5://10.26.0.189:9050\\")\\n    options.add_argument(\\"--disable-blink-features=AutomationControlled\\")\\n\\n    driver = webdriver.Remote(\\n        command_executor=f\\"http://{ip}/callisto\\",\\n        options=options,\\n        desired_capabilities=capabilities)\\n\\n    return driver","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"b152beca-de2a-42ba-a62c-b4e34f932f1c","name":"define_crawl_class","code":"class crawl:\\n    def __init__(self, galleryId):\\n        self.galleryId = galleryId\\n    \\n    def allocate_driver(self, ip):\\n        self.driver = get_selenium_driver(ip)\\n        \\n    def delocate_driver(self):\\n        self.driver.close()\\n        self.driver.quit()\\n    \\n    def articleList(self, page):\\n        id = self.galleryId\\n        url = f\\"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend\\"\\n        self.driver.get(url)\\n        self.driver.implicitly_wait(3)\\n        time.sleep(1)\\n        \\n        article_list = self.driver.find_element(By.TAG_NAME, \\"tbody\\").find_elements(By.TAG_NAME, \\"tr\\")\\n        return article_list\\n    \\n    def articleLink(self, article_list):\\n        title_link = []\\n        \\n        try:\\n            for item in article_list:\\n\\n                title_item = item.find_element(By.TAG_NAME, \\"a\\")\\n                title_item = title_item.get_attribute(\'href\')\\n                if(title_item is None or self.galleryId not in title_item):\\n                    continue\\n\\n                title_link.append(title_item)\\n                print(\\"new title link added at title_link[\\",len(title_link)-1, \\"]:\\", title_item)\\n        \\n        except:\\n            return title_link\\n    \\n        return title_link\\n                \\n    \\n    def articleText(self, crawledLink, minLen):\\n        text_list = []\\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n        \\n        \\n        for link in crawledLink:\\n            print(\\"==============START====================\\")\\n            self.driver.get(link)\\n            self.driver.implicitly_wait(10)\\n            time.sleep(2)\\n            sentence = [\\"\\",\\"\\"] # 본문, 제목\\n            \\n            article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n            try:\\n                article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n                \\n            except:\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                except:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n            \\n    \\n            \\n            boolFound = False\\n            \\n            for i in article_text:\\n                text = i.text\\n                \\n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                if len(urls) > 0:\\n                    print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                    for str_link in urls:\\n                        text = text.replace(str_link, \\"\\")\\n                    boolFound = False\\n                    \\n                if \\"이미지 순서 ON\\" in text:\\n                    print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = True\\n                    continue\\n                \\n                if \\"- dc official App\\" in text:\\n                    print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                    \\n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                    print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                    continue\\n                \\n                if text in sentence:\\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                \\n                boolFound = False\\n                text = text.replace(\\"\\\\n\\",\\" \\")\\n                print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n                    \\n                sentence[0] += text\\n\\n            if(len(sentence[0]) < minLen):\\n                print(\\"article text not added! (len < minLen) | link : \\",link)\\n                print(\\"the text is :\\", sentence[0])\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                continue\\n\\n            sentence[1] = self.driver.find_element(By.CLASS_NAME, \\"title_subject\\").text\\n            print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n            print(sentence)\\n            text_list.append(sentence)\\n            print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n            \\n        return text_list\\n    \\n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\\n    # try:\\n        text_list = []\\n        for link in tqdm.tqdm(crawledLink):\\n            try_n = 0\\n            while try_n < 2:\\n                try:\\n                    print(\\"==============START====================\\")\\n                    self.driver.get(link)\\n                    self.driver.implicitly_wait(10)\\n                    time.sleep(2)\\n\\n                    try:\\n                        articleDateHour = self.driver.find_element(By.CLASS_NAME, \\"gall_date\\").text.split(\\" \\")\\n                    except:\\n                        time.sleep(2)\\n                        articleDateHour = self.driver.find_element(By.CLASS_NAME, \\"gall_date\\").text.split(\\" \\")\\n\\n                    articleDate = articleDateHour[0].split(\\".\\")\\n                    articleHour = articleDateHour[1].split(\\":\\")\\n                    articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[1]),int(articleHour[2]), 0).timestamp()\\n\\n                    if(limUnixDate > articleUnixDate):\\n                        print(\\"too old articleText!\\")\\n                        print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                        raise\\n\\n                    sentence = [\\"\\",articleUnixDate,\\"\\"]\\n\\n                    article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n                    try:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                        checkRaise = True\\n                        for i in article_text:\\n                            if i.text == \'\':\\n                                continue\\n                            else:\\n                                checkRaise = False\\n                                break\\n\\n                        if checkRaise:\\n                            raise\\n\\n                    except:\\n                        try:\\n                            article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n                            article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n\\n                        except:\\n                            article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n\\n\\n\\n                    boolFound = False\\n\\n                    for i in article_text:\\n                        text = i.text\\n\\n                        urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                        if len(urls) > 0:\\n                            print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                            for str_link in urls:\\n                                text = text.replace(str_link, \\"\\")\\n                            boolFound = False\\n\\n                        if \\"이미지 순서 ON\\" in text:\\n                            print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                            boolFound = True\\n                            continue\\n\\n                        if \\"- dc official App\\" in text:\\n                            print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                            boolFound = False\\n                            continue\\n\\n                        if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                            print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                            continue\\n\\n                        if text in sentence:\\n                            print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                            boolFound = False\\n                            continue\\n\\n                        boolFound = False\\n                        text = text.replace(\\"\\\\n\\",\\" \\")\\n                        print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n\\n                        sentence[0] += text\\n\\n                    if(len(sentence[0]) < minLen):\\n                        print(\\"article text not added! (len < minLen) | link : \\",link)\\n                        print(\\"the text is :\\", sentence)\\n                        print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                        break\\n\\n                    sentence[2] = self.driver.find_element(By.CLASS_NAME, \\"title_subject\\").text\\n                    print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n                    print(sentence)\\n                    text_list.append(sentence)\\n                    print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                    break\\n        \\n                except Exception as e:\\n                    try_n += 1\\n                    print(e)\\n                    print(f\\"다시 시도합니다 ... (try_n : {try_n})\\")\\n        \\n        return text_list\\n#     except Exception as e:\\n\\n#         print(\\"error happend\\\\n\\", e)","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"d622d1a0-1ca8-43ce-891d-d01b108f6afb","name":"define_main_function","code":"def get_limit_date(year, month, day, hour, minute, second):\\n    limDate = [year, month, day]\\n    limHour = [hour, minute, second]\\n    limUnixDateTime = datetime.datetime(int(limDate[0]), int(limDate[1]), int(limDate[2]), int(limHour[0]), int(limHour[1]),int(limHour[2]), 0).timestamp()\\n    return limUnixDateTime\\n\\ndef get_page_content(c, page, limUnixDatetime):\\n    links = c.articleLink(c.articleList(page))\\n    text = c.articleText_withLimit(links,10,limUnixDatetime)\\n    return text\\n\\ndef create_document(data, min_len):\\n    title = data[2]\\n    timestamp = data[1]\\n    body = data[0]\\n    \\n    if len(body) < min_len:\\n        return None\\n\\n    sentences = requests.get(\\n                                \\"https://api.kshs.dev/analyze\\",\\n                                 params={\\"sentence\\": body, \\"clean\\": \\"true\\", \\"plural\\": \\"true\\"}\\n                            ).json()\\n    \\n    sentence_id = 0\\n    sentence_arr = []\\n    \\n    cons = 0\\n    pros = 0\\n    for sentence in sentences:\\n        if len(sentence[0]) < 10:\\n            tend = 2\\n            \\n        else:\\n            tend = sentence[1]\\n            if tend == 0:\\n                cons += 1\\n            \\n            elif tend == 1:\\n                pros += 1\\n            \\n        sentence_arr.append({\\n            \\"id\\": sentence_id,\\n            \\"text\\": sentence[0],\\n            \\"tend\\": tend\\n        })\\n        \\n        sentence_id += 1\\n        \\n    if cons == pros:\\n        article_tend = 2\\n    \\n    elif cons > pros:\\n        article_tend = 0\\n        \\n    elif cons < pros:\\n        article_tend = 1\\n        \\n    else:\\n        pass\\n        \\n    uname = str(timestamp)+title\\n    return {\\n        \\"_id\\": hashlib.sha256(uname.encode()).hexdigest(),\\n        \\"title\\": title,\\n        \\"timestamp\\": timestamp,\\n        \\"tend\\": article_tend,\\n        \\"cons\\": cons,\\n        \\"pros\\": pros,\\n        \\"body\\": sentence_arr\\n    }\\n\\ndef insert_document(document, cur):\\n    cur.update_one(\\n        {\\"_id\\": document[\'_id\']},\\n        {\\"$set\\": document},\\n        upsert = True\\n    )\\n    ","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"ca759fa5-682e-4cbe-af92-0f6de312f289","name":"init_class","code":"gallId = \\"newconservativeparty\\"\\nlimUnixDatetime = get_limit_date(2022, 8, 22, 12, 0, 0)\\nminPage = 3\\nmaxPage = 10\\nip = \\"10.99.151.246\\"","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}}],"edges":[{"parent":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","child":"0d487371-c4fc-44ed-a77e-87b5b293556f"},{"parent":"0d487371-c4fc-44ed-a77e-87b5b293556f","child":"b152beca-de2a-42ba-a62c-b4e34f932f1c"},{"parent":"b152beca-de2a-42ba-a62c-b4e34f932f1c","child":"d622d1a0-1ca8-43ce-891d-d01b108f6afb"},{"parent":"d622d1a0-1ca8-43ce-891d-d01b108f6afb","child":"ca759fa5-682e-4cbe-af92-0f6de312f289"}]}\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666720390.36.1.1666720653.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666720390.36.1.1666720653.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev')
[2022-10-25 18:22:49] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'name': 'import_packages', 'code': 'from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm\nfrom pprint import pprint\nimport ray\nfrom pymongo import MongoClient\nimport hashlib', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'name': 'define_driver', 'code': 'def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'b152beca-de2a-42ba-a62c-b4e34f932f1c', 'name': 'define_crawl_class', 'code': 'class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n    \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocate_driver(self):\n        self.driver.close()\n        self.driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n                \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ["",""] # 본문, 제목\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence[0] += text\n\n            if(len(sentence[0]) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence[0])\n                print("============END======================\\n\\n\\n")\n                continue\n\n            sentence[1] = self.driver.find_element(By.CLASS_NAME, "title_subject").text\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n    # try:\n        text_list = []\n        for link in tqdm.tqdm(crawledLink):\n            try_n = 0\n            while try_n < 2:\n                try:\n                    print("==============START====================")\n                    self.driver.get(link)\n                    self.driver.implicitly_wait(10)\n                    time.sleep(2)\n\n                    try:\n                        articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                    except:\n                        time.sleep(2)\n                        articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n\n                    articleDate = articleDateHour[0].split(".")\n                    articleHour = articleDateHour[1].split(":")\n                    articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[1]),int(articleHour[2]), 0).timestamp()\n\n                    if(limUnixDate > articleUnixDate):\n                        print("too old articleText!")\n                        print("============END======================\\n\\n\\n")\n                        raise\n\n                    sentence = ["",articleUnixDate,""]\n\n                    article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                        checkRaise = True\n                        for i in article_text:\n                            if i.text == \'\':\n                                continue\n                            else:\n                                checkRaise = False\n                                break\n\n                        if checkRaise:\n                            raise\n\n                    except:\n                        try:\n                            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                            article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                        except:\n                            article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                    boolFound = False\n\n                    for i in article_text:\n                        text = i.text\n\n                        urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                        if len(urls) > 0:\n                            print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                            for str_link in urls:\n                                text = text.replace(str_link, "")\n                            boolFound = False\n\n                        if "이미지 순서 ON" in text:\n                            print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                            boolFound = True\n                            continue\n\n                        if "- dc official App" in text:\n                            print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                            boolFound = False\n                            continue\n\n                        if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                            print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                            continue\n\n                        if text in sentence:\n                            print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                            boolFound = False\n                            continue\n\n                        boolFound = False\n                        text = text.replace("\\n"," ")\n                        print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                        sentence[0] += text\n\n                    if(len(sentence[0]) < minLen):\n                        print("article text not added! (len < minLen) | link : ",link)\n                        print("the text is :", sentence)\n                        print("============END======================\\n\\n\\n")\n                        break\n\n                    sentence[2] = self.driver.find_element(By.CLASS_NAME, "title_subject").text\n                    print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                    print(sentence)\n                    text_list.append(sentence)\n                    print("============END======================\\n\\n\\n")\n                    break\n        \n                except Exception as e:\n                    try_n += 1\n                    print(e)\n                    print(f"다시 시도합니다 ... (try_n : {try_n})")\n        \n        return text_list\n#     except Exception as e:\n\n#         print("error happend\\n", e)', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'd622d1a0-1ca8-43ce-891d-d01b108f6afb', 'name': 'define_main_function', 'code': 'def get_limit_date(year, month, day, hour, minute, second):\n    limDate = [year, month, day]\n    limHour = [hour, minute, second]\n    limUnixDateTime = datetime.datetime(int(limDate[0]), int(limDate[1]), int(limDate[2]), int(limHour[0]), int(limHour[1]),int(limHour[2]), 0).timestamp()\n    return limUnixDateTime\n\ndef get_page_content(c, page, limUnixDatetime):\n    links = c.articleLink(c.articleList(page))\n    text = c.articleText_withLimit(links,10,limUnixDatetime)\n    return text\n\ndef create_document(data, min_len):\n    title = data[2]\n    timestamp = data[1]\n    body = data[0]\n    \n    if len(body) < min_len:\n        return None\n\n    sentences = requests.get(\n                                "https://api.kshs.dev/analyze",\n                                 params={"sentence": body, "clean": "true", "plural": "true"}\n                            ).json()\n    \n    sentence_id = 0\n    sentence_arr = []\n    \n    cons = 0\n    pros = 0\n    for sentence in sentences:\n        if len(sentence[0]) < 10:\n            tend = 2\n            \n        else:\n            tend = sentence[1]\n            if tend == 0:\n                cons += 1\n            \n            elif tend == 1:\n                pros += 1\n            \n        sentence_arr.append({\n            "id": sentence_id,\n            "text": sentence[0],\n            "tend": tend\n        })\n        \n        sentence_id += 1\n        \n    if cons == pros:\n        article_tend = 2\n    \n    elif cons > pros:\n        article_tend = 0\n        \n    elif cons < pros:\n        article_tend = 1\n        \n    else:\n        pass\n        \n    uname = str(timestamp)+title\n    return {\n        "_id": hashlib.sha256(uname.encode()).hexdigest(),\n        "title": title,\n        "timestamp": timestamp,\n        "tend": article_tend,\n        "cons": cons,\n        "pros": pros,\n        "body": sentence_arr\n    }\n\ndef insert_document(document, cur):\n    cur.update_one(\n        {"_id": document[\'_id\']},\n        {"$set": document},\n        upsert = True\n    )\n    ', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'ca759fa5-682e-4cbe-af92-0f6de312f289', 'name': 'init_class', 'code': 'gallId = "newconservativeparty"\nlimUnixDatetime = get_limit_date(2022, 8, 22, 12, 0, 0)\nminPage = 3\nmaxPage = 10\nip = "10.99.151.246"', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}], 'edges': [{'parent': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'child': '0d487371-c4fc-44ed-a77e-87b5b293556f'}, {'parent': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'child': 'b152beca-de2a-42ba-a62c-b4e34f932f1c'}, {'parent': 'b152beca-de2a-42ba-a62c-b4e34f932f1c', 'child': 'd622d1a0-1ca8-43ce-891d-d01b108f6afb'}, {'parent': 'd622d1a0-1ca8-43ce-891d-d01b108f6afb', 'child': 'ca759fa5-682e-4cbe-af92-0f6de312f289'}]}
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear init_class (ca759fa5-682e-4cbe-af92-0f6de312f289), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_main_function (d622d1a0-1ca8-43ce-891d-d01b108f6afb), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear init_class (ca759fa5-682e-4cbe-af92-0f6de312f289), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_main_function (d622d1a0-1ca8-43ce-891d-d01b108f6afb), status MRXLinkComponentStatus.INVALID
[2022-10-25 18:22:49] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear init_class (ca759fa5-682e-4cbe-af92-0f6de312f289), status MRXLinkComponentStatus.INVALID

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alsruf36/political-disposition-determiner/blob/master/notebooks/political_disposition_determiner_evaluater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCM_mlATVCZ"
      },
      "source": [
        "1. Colab 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "path = \"/gdrive/MyDrive/Colab Notebooks/pickles\""
      ],
      "metadata": {
        "id": "PFpd9aXXB1j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43cZGvKwS_zR"
      },
      "outputs": [],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch\n",
        "!pip install kss\n",
        "!pip install konlpy\n",
        "!pip install anvil-uplink\n",
        "!pip install textrankr \n",
        "!pip install typing\n",
        "!pip install dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeipNhmUTJPR"
      },
      "outputs": [],
      "source": [
        "#깃허브에서 KoBERT 파일 로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JZykKWx0TKzg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import kss\n",
        "from pprint import pprint\n",
        "import emoji\n",
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bzdo6WzHTMBN"
      },
      "outputs": [],
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WoIOvsawTNK2"
      },
      "outputs": [],
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuwSOsjhTOAA"
      },
      "outputs": [],
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zaszHW7eTt4z"
      },
      "outputs": [],
      "source": [
        "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "Cdtyc2Y4NdkF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3-lagW0oT17z"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=2,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI0xCq2JUAlD"
      },
      "source": [
        "6. KoBERT 모델 학습시키기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "\n",
        "with open(path + '/model-100000-50000-50000-64-10.pkl', 'rb') as f:\n",
        "    model = dill.load(f)"
      ],
      "metadata": {
        "id": "eMsF04o-wQQh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh7B17vMUFWA"
      },
      "source": [
        "7. 새로운 문장 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EY9YiCd5UF2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400cdb39-b7a8-4509-c5b7-b240217ef61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ],
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n_v94-X_UGuq"
      },
      "outputs": [],
      "source": [
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(0)\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(1)\n",
        "\n",
        "        print(\">> 입력하신 내용에서 \" + str(test_eval[0]) + \" 느껴집니다.\")\n",
        "        return test_eval[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anvil.server\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from typing import List\n",
        "from textrankr import TextRank\n",
        "import random\n",
        "\n",
        "anvil.server.connect(\"server_YEW72CBQZBHHFVTJEFQLAQEH-BMIRAKWNH5DZNLW4\", url=\"ws://mingyeol.com:1303/_/uplink\")\n",
        "\n",
        "class ArticleListCrawler:\n",
        "  def __init__(self, count):\n",
        "      custom_header = {\n",
        "          'referer' : 'https://www.naver.com/',\n",
        "          'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
        "      }\n",
        "      \n",
        "      list_href = []\n",
        "      result = []\n",
        "      \n",
        "      section = \"정치\"\n",
        "      req = self.get_request(section)\n",
        "      soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "      \n",
        "      list_href = self.get_href(soup)\n",
        "\n",
        "      target = random.randrange(0, count)\n",
        "      target_url = list_href[target]\n",
        "      oid = target_url.split('&')[3].split('=')[1]\n",
        "      aid = target_url.split('&')[4].split('=')[1]\n",
        "      \n",
        "      self.ids = {\n",
        "          \"oid\": oid,\n",
        "          \"aid\": aid\n",
        "      }\n",
        "\n",
        "  def get_href(self, soup) :\n",
        "      result = []\n",
        "      \n",
        "      div = soup.find(\"div\", class_=\"list_body newsflash_body\")\n",
        "      \n",
        "      for dt in div.find_all(\"dt\", class_=\"photo\"):\n",
        "          result.append(dt.find(\"a\")[\"href\"])\n",
        "      \n",
        "      return result\n",
        "\n",
        "  def get_request(self, section) :\n",
        "      custom_header = {\n",
        "          'referer' : 'https://www.naver.com/',\n",
        "          'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
        "      }\n",
        "      \n",
        "      url = \"https://news.naver.com/main/list.nhn\"\n",
        "      \n",
        "      sections = {\n",
        "          \"정치\" : 100,\n",
        "          \"경제\" : 101,\n",
        "          \"사회\" : 102,\n",
        "          \"생활\" : 103,\n",
        "          \"세계\" : 104,\n",
        "          \"과학\" : 105\n",
        "      }\n",
        "      \n",
        "      req = requests.get(url, headers = custom_header,\n",
        "              params = {\"sid1\" : sections[section]})\n",
        "      \n",
        "      return req\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __call__(self, text: str) -> List[str]:\n",
        "        tokens: List[str] = text.split()\n",
        "        return tokens\n",
        "\n",
        "def summarize(text, line):\n",
        "    mytokenizer: MyTokenizer = MyTokenizer()\n",
        "    textrank: TextRank = TextRank(mytokenizer)\n",
        "\n",
        "    summaries: List[str] = textrank.summarize(text, line, verbose=False)\n",
        "    return summaries\n",
        "\n",
        "@anvil.server.callable\n",
        "def getArticle():\n",
        "  ids = ArticleListCrawler(20).ids\n",
        "  url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid={}&aid={}'.format(ids['oid'], ids['aid'])\n",
        "  r = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "  soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "  title = soup.select_one('h3#articleTitle').text\n",
        "  content = soup.select_one('#articleBodyContents')\n",
        "  subtitle = content.select_one('strong')\n",
        "  if subtitle is not None: subtitle = subtitle.extract().text\n",
        "\n",
        "  for x in content.select('script'): x.extract()                             # <script>...</script> 제거\n",
        "  for x in content(text=lambda text: isinstance(text, Comment)): x.extract() # <!-- 주석 --> 제거\n",
        "  for x in content.select(\"br\"): x.replace_with(\"\\n\")                        # <br>을 \\n로 교체\n",
        "  for x in content.find_all('span'): x.decompose()                           # 기타 span 제거\n",
        "  for x in content.find_all('div'): x.decompose()                            # 기타 div 제거\n",
        "  content = \"\".join([str(x) for x in content.contents])                      # 최상위 태그 제거(=innerHtml 추출)\n",
        "  content = content.strip()                                                  # 앞뒤 공백 제거\n",
        "  content = kss.split_sentences(content)                                     # 문장 단위로 분리\n",
        "  summary = summarize(\"\\n\".join(content), 5)                                 # 문장 요약\n",
        "\n",
        "  return {\n",
        "      \"title\": title,\n",
        "      \"subtitle\": subtitle,\n",
        "      \"content\": content,\n",
        "      \"summary\": summary\n",
        "  }\n",
        "\n",
        "@anvil.server.callable\n",
        "def predicter(text):\n",
        "  texts = kss.split_sentences(text)\n",
        "  result = [[x, predict(x)] for x in texts]\n",
        "\n",
        "  return result\n",
        "\n",
        "anvil.server.wait_forever()"
      ],
      "metadata": {
        "id": "blAZxPdKfBAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "political-disposition-determiner-evaluater.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMk7EgM8rkXCH5iDLc6Q+6o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
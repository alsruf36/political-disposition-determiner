{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alsruf36/political-disposition-determiner/blob/master/notebooks/political_disposition_determiner_modeler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCM_mlATVCZ"
      },
      "source": [
        "1. Colab 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43cZGvKwS_zR"
      },
      "outputs": [],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch\n",
        "!pip install kss\n",
        "!pip install konlpy\n",
        "!pip install textrankr \n",
        "!pip install typing\n",
        "!pip install dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeipNhmUTJPR"
      },
      "outputs": [],
      "source": [
        "#깃허브에서 KoBERT 파일 로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZykKWx0TKzg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import kss\n",
        "from pprint import pprint\n",
        "import emoji\n",
        "from konlpy.tag import Okt\n",
        "from hanspell import spell_checker\n",
        "from soynlp.normalizer import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzdo6WzHTMBN"
      },
      "outputs": [],
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoIOvsawTNK2"
      },
      "outputs": [],
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuwSOsjhTOAA"
      },
      "outputs": [],
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5t9RRI-TPxN"
      },
      "source": [
        "2. 데이터셋 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DssNUiDeWgDH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "URL = \"http://mingyeol.com:5000/comment\" #댓글 요청을 위한 URL\n",
        "\n",
        "#파라미터에서 count는 꼭 포함해야 하며, count 외에는 필요한 조건만 포함해야한다.\n",
        "params = {\n",
        "    \"count\": 100000,            #불러올 댓글 개수\n",
        "    #\"tend\": \"pro\",             #정치 성향 (con : 보수 | pro : 진보)\n",
        "    \"minlike\": 20,              #댓글의 최소 공감 개수\n",
        "    \"minlength\": 20,            #댓글의 최소 길이\n",
        "    \"mintimestamp\": 1514732400, #댓글의 최소 날짜 (Unix Time 형식)\n",
        "    #\"level\": 3                 #언론사의 레벨 (0부터 3까지 이며 그 숫자 이하 레벨의 모든 언론사를 대상으로 한다. 숫자가 작을수록 극좌/극우에 가깝다.)\n",
        "}\n",
        "\n",
        "response = requests.get(URL, params=params) #requests 모듈을 통해 API에 요청\n",
        "comments = json.loads(response.text) #로드된 JSON 텍스트를 배열로 변경\n",
        "contents = [[x['normalized'], x['calculate']] for x in comments] #comments 중 원하는 항목만 추출\n",
        "\n",
        "cons = [x for x in contents if x[1] == 0] #보수 댓글 리스트\n",
        "pros = [x for x in contents if x[1] == 1] #진보 댓글 리스트\n",
        "\n",
        "\n",
        "print(\"{}개의 데이터를 성공적으로 불러왔습니다. (보수 {}개 | 진보 {}개)\".format(len(contents), len(cons), len(pros)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9szM0b6TmYQ"
      },
      "source": [
        "3. Train data & Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8ifGT0hn2fu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c23g2weMTnhd"
      },
      "outputs": [],
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "                                                         \n",
        "dataset_train, dataset_test = train_test_split(contents, test_size=0.25, random_state=0)\n",
        "\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sNj24iTs2G"
      },
      "source": [
        "4. KoBERT 입력 데이터로 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zaszHW7eTt4z"
      },
      "outputs": [],
      "source": [
        "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "S3T6-CQvTusW"
      },
      "outputs": [],
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muMQQjKrTvop"
      },
      "outputs": [],
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ny7GTeBTwcq"
      },
      "outputs": [],
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
        "\n",
        "\n",
        "print(data_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "P9v_XtG4TxQx"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAgxJ4VGT1F6"
      },
      "source": [
        "5. KoBERT 학습모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3-lagW0oT17z"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=2,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "JHeKK2RjT4LW"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "w1ZagVE5T4-P"
      },
      "outputs": [],
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4g4ObMhgT6H7"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "u7YqXaSZT6-F"
      },
      "outputs": [],
      "source": [
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "XQjSh5MnT7wv"
      },
      "outputs": [],
      "source": [
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "NUdJiX0TT-cp"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os38lpNyT_u4"
      },
      "outputs": [],
      "source": [
        "train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI0xCq2JUAlD"
      },
      "source": [
        "6. KoBERT 모델 학습시키기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1gxH5MbUBVW"
      },
      "outputs": [],
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm.notebook.tqdm(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm.notebook.tqdm(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMsF04o-wQQh"
      },
      "outputs": [],
      "source": [
        "import dill\n",
        "from google.colab import drive\n",
        "import datetime as pydatetime\n",
        "\n",
        "def get_now_timestamp():\n",
        "    return int(pydatetime.datetime.now().timestamp())\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "path = \"/gdrive/MyDrive/Colab Notebooks/pickles\"\n",
        "\n",
        "with open(path + '/model-{}-{}-{}-{}-{}-{}.pkl'.format(len(contents), len(cons), len(pros), batch_size, num_epochs, get_now_timestamp()), 'wb') as f:\n",
        "    dill.dump(model, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "political-disposition-determiner-modeler.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkKXNZ8uILhAl/x+aR2LPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
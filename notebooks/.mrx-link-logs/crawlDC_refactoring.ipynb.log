[2022-10-25 13:00:59] mrx-link.MRXLinkMagics.mrxlink_set_parameters() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='[]\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702840.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702840.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev', no_reply=True)
[2022-10-25 13:00:59] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='{"nodes":[{"id":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","name":"import_packages","code":"from selenium import webdriver\\nfrom selenium.webdriver.chrome.options import Options\\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\\nimport time\\nimport re\\nimport dill as pickle\\nimport datetime\\nimport requests\\nimport json\\nimport tqdm","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"0d487371-c4fc-44ed-a77e-87b5b293556f","name":"define_driver","code":"def get_selenium_driver(ip):\\n    capabilities = {\\n        \\"browserName\\": \\"chrome\\",\\n        \\"browserVersion\\": \\"latest\\",\\n        \\"selenoid:options\\": {\\n            \\"enableVNC\\": True,\\n            \\"enableVideo\\": False\\n        }\\n    }\\n\\n    options = Options()\\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\\n    options.add_experimental_option(\\"excludeSwitches\\", [\\"enable-automation\\"])\\n    options.add_experimental_option(\'useAutomationExtension\', False)\\n    #options.add_argument(\\"--proxy-server=socks5://10.26.0.189:9050\\")\\n    options.add_argument(\\"--disable-blink-features=AutomationControlled\\")\\n\\n    driver = webdriver.Remote(\\n        command_executor=f\\"http://{ip}/callisto\\",\\n        options=options,\\n        desired_capabilities=capabilities)\\n\\n    return driver","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","name":"define_pickle","code":"def save_pickle(data, path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'wb\') as f:\\n        pickle.dump(data, f)\\n\\ndef load_pickle(path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'rb\') as f:\\n        data = pickle.load(f)\\n\\n    return data","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"b152beca-de2a-42ba-a62c-b4e34f932f1c","name":"define_crawl_class","code":"class crawl:\\n    def __init__(self, galleryId):\\n        self.galleryId = galleryId\\n        \\n    def allocate_driver(self, ip):\\n        self.driver = get_selenium_driver(ip)\\n        \\n    def delocalte_driver(self):\\n        driver.close()\\n        driver.quit()\\n    \\n    def articleList(self, page):\\n        id = self.galleryId\\n        url = f\\"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend\\"\\n        self.driver.get(url)\\n        self.driver.implicitly_wait(3)\\n        time.sleep(1)\\n        \\n        article_list = self.driver.find_element(By.TAG_NAME, \\"tbody\\").find_elements(By.TAG_NAME, \\"tr\\")\\n        return article_list\\n    \\n    def articleLink(self, article_list):\\n        title_link = []\\n        \\n        try:\\n            for item in article_list:\\n\\n                title_item = item.find_element(By.TAG_NAME, \\"a\\")\\n                title_item = title_item.get_attribute(\'href\')\\n                if(title_item is None or self.galleryId not in title_item):\\n                    continue\\n\\n                title_link.append(title_item)\\n                print(\\"new title link added at title_link[\\",len(title_link)-1, \\"]:\\", title_item)\\n        \\n        except:\\n            return title_link\\n    \\n        return title_link\\n    \\n    \\n    \\n    def articleText(self, crawledLink, minLen):\\n        text_list = []\\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n        \\n        \\n        for link in crawledLink:\\n            print(\\"==============START====================\\")\\n            self.driver.get(link)\\n            self.driver.implicitly_wait(10)\\n            time.sleep(2)\\n            sentence = \\"\\"\\n            \\n            article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n            try:\\n                article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n                \\n            except:\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                except:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n            \\n    \\n            \\n            boolFound = False\\n            \\n            for i in article_text:\\n                text = i.text\\n                \\n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                if len(urls) > 0:\\n                    print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                    for str_link in urls:\\n                        text = text.replace(str_link, \\"\\")\\n                    boolFound = False\\n                    \\n                if \\"이미지 순서 ON\\" in text:\\n                    print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = True\\n                    continue\\n                \\n                if \\"- dc official App\\" in text:\\n                    print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                    \\n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                    print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                    continue\\n                \\n                if text in sentence:\\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                \\n                boolFound = False\\n                text = text.replace(\\"\\\\n\\",\\" \\")\\n                print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n                    \\n                sentence += text\\n\\n            if(len(sentence) < minLen):\\n                print(\\"article text not added! (len < minLen) | link : \\",link)\\n                print(\\"the text is :\\", sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                continue\\n\\n\\n            print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n            print(sentence)\\n            text_list.append(sentence)\\n            print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n            \\n        return text_list\\n    \\n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\\n        try:\\n            text_list = []\\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n\\n\\n            for link in crawledLink:\\n                print(\\"==============START====================\\")\\n                self.driver.get(link)\\n                self.driver.implicitly_wait(10)\\n                time.sleep(2)\\n\\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, \\"gall_date\\").text.split(\\" \\")\\n                articleDate = articleDateHour[0].split(\\".\\")\\n                articleHour = articleDateHour[1].split(\\":\\")\\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\\n\\n\\n                if(limUnixDate > articleUnixDate):\\n                    continue\\n\\n                sentence = \\"\\"\\n\\n                article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n\\n                except:\\n                    try:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                    except:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n\\n\\n\\n                boolFound = False\\n\\n                for i in article_text:\\n                    text = i.text\\n\\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                    if len(urls) > 0:\\n                        print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                        for str_link in urls:\\n                            text = text.replace(str_link, \\"\\")\\n                        boolFound = False\\n\\n                    if \\"이미지 순서 ON\\" in text:\\n                        print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = True\\n                        continue\\n\\n                    if \\"- dc official App\\" in text:\\n                        print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                        print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                        continue\\n\\n                    if text in sentence:\\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    boolFound = False\\n                    text = text.replace(\\"\\\\n\\",\\" \\")\\n                    print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n\\n                    sentence += text\\n\\n                if(len(sentence) < minLen):\\n                    print(\\"article text not added! (len < minLen) | link : \\",link)\\n                    print(\\"the text is :\\", sentence)\\n                    print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                    continue\\n\\n\\n                print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n                print(sentence)\\n                text_list.append(sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n\\n            return text_list\\n        except Exception as e:\\n            \\n            print(\\"error happend\\\\n\\", e)","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}}],"edges":[{"parent":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","child":"0d487371-c4fc-44ed-a77e-87b5b293556f"},{"parent":"0d487371-c4fc-44ed-a77e-87b5b293556f","child":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55"},{"parent":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","child":"b152beca-de2a-42ba-a62c-b4e34f932f1c"}]}\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702840.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702840.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev')
[2022-10-25 13:00:59] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'name': 'import_packages', 'code': 'from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'name': 'define_driver', 'code': 'def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'name': 'define_pickle', 'code': 'def save_pickle(data, path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'wb\') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'rb\') as f:\n        data = pickle.load(f)\n\n    return data', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'b152beca-de2a-42ba-a62c-b4e34f932f1c', 'name': 'define_crawl_class', 'code': 'class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in crawledLink:\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}], 'edges': [{'parent': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'child': '0d487371-c4fc-44ed-a77e-87b5b293556f'}, {'parent': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'child': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55'}, {'parent': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'child': 'b152beca-de2a-42ba-a62c-b4e34f932f1c'}]}
[2022-10-25 13:00:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:00:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:00:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm\n', id='455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', name='Unknown', type='CodeCell')
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:00] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:01] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:02] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf): {'Options': "<class 'selen...ions.Options'>", 'By': "<class 'selen...common.by.By'>", 'datetime': "<module 'date.../datetime.py'>", 'json': "<module 'json.../__init__.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'pickle': "<module 'dill.../__init__.py'>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>"}
[2022-10-25 13:01:08] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm\n', id='455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', name='Unknown', type='CodeCell')
[2022-10-25 13:01:08] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:08] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:08] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:08] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:08] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:08] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:08] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:08] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf): {'Options': "<class 'selen...ions.Options'>", 'By': "<class 'selen...common.by.By'>", 'datetime': "<module 'date.../datetime.py'>", 'json': "<module 'json.../__init__.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'pickle': "<module 'dill.../__init__.py'>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>"}
[2022-10-25 13:01:09] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver\n', id='0d487371-c4fc-44ed-a77e-87b5b293556f', name='Unknown', type='CodeCell')
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f): {'By': "<class 'selen...common.by.By'>", 'get_selenium_driver': '<function run...x7fa5b4266710>', 'Options': "<class 'selen...ions.Options'>", 'datetime': "<module 'date.../datetime.py'>", 'json': "<module 'json.../__init__.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'pickle': "<module 'dill.../__init__.py'>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>"}
[2022-10-25 13:01:09] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='def save_pickle(data, path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'wb\') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'rb\') as f:\n        data = pickle.load(f)\n\n    return data\n', id='e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', name='Unknown', type='CodeCell')
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:09] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55): {'By': "<class 'selen...common.by.By'>", 'get_selenium_driver': '<function run...x7fa58c91e8c0>', 'Options': "<class 'selen...ions.Options'>", 'save_pickle': '<function run...x7fa51debf8c0>', 'load_pickle': '<function run...x7fa51debf950>', 'datetime': "<module 'date.../datetime.py'>", 'json': "<module 'json.../__init__.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>", 'pickle': "<module 'dill.../__init__.py'>"}
[2022-10-25 13:01:09] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in crawledLink:\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)\n', id='b152beca-de2a-42ba-a62c-b4e34f932f1c', name='Unknown', type='CodeCell')
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:01:10] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c): {'Options': "<class 'selen...ions.Options'>", 'save_pickle': '<function run...x7fa51df8ecb0>', 'load_pickle': '<function run...x7fa51dee70e0>', 'crawl': "<class 'execu...ocals>.crawl'>", 'By': "<class 'selen...common.by.By'>", 'get_selenium_driver': '<function run...x7fa58c5bce60>', 'json': "<module 'json.../__init__.py'>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>", 'pickle': "<module 'dill.../__init__.py'>", 'datetime': "<module 'date.../datetime.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>"}
[2022-10-25 13:02:45] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in tqdm.tqdm(crawledLink):\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)\n', id='b152beca-de2a-42ba-a62c-b4e34f932f1c', name='Unknown', type='CodeCell')
[2022-10-25 13:02:45] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: old code: class crawl:
    def __init__(self, galleryId):
        self.galleryId = galleryId

    def allocate_driver(self, ip):
        self.driver = get_selenium_driver(ip)

    def delocalte_driver(self):
        driver.close()
        driver.quit()

    def articleList(self, page):
        id = self.galleryId
        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"
        self.driver.get(url)
        self.driver.implicitly_wait(3)
        time.sleep(1)
        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")
        return article_list

    def articleLink(self, article_list):
        title_link = []
        try:
            for item in article_list:
                title_item = item.find_element(By.TAG_NAME, "a")
                title_item = title_item.get_attribute("href")
                if title_item is None or self.galleryId not in title_item:
                    continue
                title_link.append(title_item)
                print("new title link added at title_link[", len(title_link) - 1, "]:", title_item)
        except:
            return title_link
        return title_link

    def articleText(self, crawledLink, minLen):
        text_list = []
        num = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
        for link in crawledLink:
            print("==============START====================")
            self.driver.get(link)
            self.driver.implicitly_wait(10)
            time.sleep(2)
            sentence = ""
            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")
            try:
                article_text = article_text.find_elements(By.TAG_NAME, "p")
            except:
                try:
                    article_text = article_text.find_elements(By.TAG_NAME, "div")
                except:
                    article_text = article_text.find_elements(By.TAG_NAME, "span")
            boolFound = False
            for i in article_text:
                text = i.text
                urls = re.findall(
                    "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", i.text
                )
                if len(urls) > 0:
                    print(f"url exist, replaced. the text is :{text}\n=======")
                    for str_link in urls:
                        text = text.replace(str_link, "")
                    boolFound = False
                if "이미지 순서 ON" in text:
                    print(f'"이미지 순서 ON" exist, replaced. the text is :{text}\n=======')
                    boolFound = True
                    continue
                if "- dc official App" in text:
                    print(f'"- dc official App" exist, replaced. the text is :{text}\n=======')
                    boolFound = False
                    continue
                if boolFound is True and ((text >= "0" and text <= "9") or " " in text):
                    print(f'number after "이미지 순서 ON", the text is :{text}\n=======')
                    continue
                if text in sentence:
                    print(f"text already exist, text is {text} | sentence is {sentence}\n=======")
                    boolFound = False
                    continue
                boolFound = False
                text = text.replace("\n", " ")
                print(f"removed every trash data at text, the text is :{text}\n=======")
                sentence += text
            if len(sentence) < minLen:
                print("article text not added! (len < minLen) | link : ", link)
                print("the text is :", sentence)
                print("============END======================\n\n\n")
                continue
            print("article text added at text_list[", len(text_list), "]", "(link :", link, ") :")
            print(sentence)
            text_list.append(sentence)
            print("============END======================\n\n\n")
        return text_list

    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):
        try:
            text_list = []
            num = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
            for link in crawledLink:
                print("==============START====================")
                self.driver.get(link)
                self.driver.implicitly_wait(10)
                time.sleep(2)
                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")
                articleDate = articleDateHour[0].split(".")
                articleHour = articleDateHour[1].split(":")
                articleUnixDate = datetime.datetime(
                    int(articleDate[0]),
                    int(articleDate[1]),
                    int(articleDate[2]),
                    int(articleHour[0]),
                    int(articleHour[0]),
                    int(articleHour[0]),
                    0,
                ).timestamp()
                if limUnixDate > articleUnixDate:
                    continue
                sentence = ""
                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")
                try:
                    article_text = article_text.find_elements(By.TAG_NAME, "p")
                except:
                    try:
                        article_text = article_text.find_elements(By.TAG_NAME, "div")
                    except:
                        article_text = article_text.find_elements(By.TAG_NAME, "span")
                boolFound = False
                for i in article_text:
                    text = i.text
                    urls = re.findall(
                        "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", i.text
                    )
                    if len(urls) > 0:
                        print(f"url exist, replaced. the text is :{text}\n=======")
                        for str_link in urls:
                            text = text.replace(str_link, "")
                        boolFound = False
                    if "이미지 순서 ON" in text:
                        print(f'"이미지 순서 ON" exist, replaced. the text is :{text}\n=======')
                        boolFound = True
                        continue
                    if "- dc official App" in text:
                        print(f'"- dc official App" exist, replaced. the text is :{text}\n=======')
                        boolFound = False
                        continue
                    if boolFound is True and ((text >= "0" and text <= "9") or " " in text):
                        print(f'number after "이미지 순서 ON", the text is :{text}\n=======')
                        continue
                    if text in sentence:
                        print(f"text already exist, text is {text} | sentence is {sentence}\n=======")
                        boolFound = False
                        continue
                    boolFound = False
                    text = text.replace("\n", " ")
                    print(f"removed every trash data at text, the text is :{text}\n=======")
                    sentence += text
                if len(sentence) < minLen:
                    print("article text not added! (len < minLen) | link : ", link)
                    print("the text is :", sentence)
                    print("============END======================\n\n\n")
                    continue
                print("article text added at text_list[", len(text_list), "]", "(link :", link, ") :")
                print(sentence)
                text_list.append(sentence)
                print("============END======================\n\n\n")
            return text_list
        except Exception as e:
            print("error happend\n", e)

[2022-10-25 13:02:45] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: new code: class crawl:
    def __init__(self, galleryId):
        self.galleryId = galleryId
        
    def allocate_driver(self, ip):
        self.driver = get_selenium_driver(ip)
        
    def delocalte_driver(self):
        driver.close()
        driver.quit()
    
    def articleList(self, page):
        id = self.galleryId
        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"
        self.driver.get(url)
        self.driver.implicitly_wait(3)
        time.sleep(1)
        
        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")
        return article_list
    
    def articleLink(self, article_list):
        title_link = []
        
        try:
            for item in article_list:

                title_item = item.find_element(By.TAG_NAME, "a")
                title_item = title_item.get_attribute('href')
                if(title_item is None or self.galleryId not in title_item):
                    continue

                title_link.append(title_item)
                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)
        
        except:
            return title_link
    
        return title_link
    
    
    
    def articleText(self, crawledLink, minLen):
        text_list = []
        num = ['0','1','2','3','4','5','6','7','8','9']
        
        
        for link in crawledLink:
            print("==============START====================")
            self.driver.get(link)
            self.driver.implicitly_wait(10)
            time.sleep(2)
            sentence = ""
            
            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")
            try:
                article_text = article_text.find_elements(By.TAG_NAME, "p")
                
            except:
                try:
                    article_text = article_text.find_elements(By.TAG_NAME, "div")
                except:
                    article_text = article_text.find_elements(By.TAG_NAME, "span")
            
    
            
            boolFound = False
            
            for i in article_text:
                text = i.text
                
                urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', i.text)
                if len(urls) > 0:
                    print(f'url exist, replaced. the text is :{text}\n=======')
                    for str_link in urls:
                        text = text.replace(str_link, "")
                    boolFound = False
                    
                if "이미지 순서 ON" in text:
                    print(f'"이미지 순서 ON" exist, replaced. the text is :{text}\n=======')
                    boolFound = True
                    continue
                
                if "- dc official App" in text:
                    print(f'"- dc official App" exist, replaced. the text is :{text}\n=======')
                    boolFound = False
                    continue
                    
                if boolFound is True and ((text >= '0' and text <= '9') or " " in text):
                    print(f'number after "이미지 순서 ON", the text is :{text}\n=======')
                    continue
                
                if text in sentence:
                    print(f'text already exist, text is {text} | sentence is {sentence}\n=======')
                    boolFound = False
                    continue
                
                boolFound = False
                text = text.replace("\n"," ")
                print(f'removed every trash data at text, the text is :{text}\n=======')
                    
                sentence += text

            if(len(sentence) < minLen):
                print("article text not added! (len < minLen) | link : ",link)
                print("the text is :", sentence)
                print("============END======================\n\n\n")
                continue


            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")
            print(sentence)
            text_list.append(sentence)
            print("============END======================\n\n\n")
            
        return text_list
    
    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):
        try:
            text_list = []
            num = ['0','1','2','3','4','5','6','7','8','9']


            for link in tqdm.tqdm(crawledLink):
                print("==============START====================")
                self.driver.get(link)
                self.driver.implicitly_wait(10)
                time.sleep(2)

                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")
                articleDate = articleDateHour[0].split(".")
                articleHour = articleDateHour[1].split(":")
                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()


                if(limUnixDate > articleUnixDate):
                    continue

                sentence = ""

                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")
                try:
                    article_text = article_text.find_elements(By.TAG_NAME, "p")

                except:
                    try:
                        article_text = article_text.find_elements(By.TAG_NAME, "div")
                    except:
                        article_text = article_text.find_elements(By.TAG_NAME, "span")



                boolFound = False

                for i in article_text:
                    text = i.text

                    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', i.text)
                    if len(urls) > 0:
                        print(f'url exist, replaced. the text is :{text}\n=======')
                        for str_link in urls:
                            text = text.replace(str_link, "")
                        boolFound = False

                    if "이미지 순서 ON" in text:
                        print(f'"이미지 순서 ON" exist, replaced. the text is :{text}\n=======')
                        boolFound = True
                        continue

                    if "- dc official App" in text:
                        print(f'"- dc official App" exist, replaced. the text is :{text}\n=======')
                        boolFound = False
                        continue

                    if boolFound is True and ((text >= '0' and text <= '9') or " " in text):
                        print(f'number after "이미지 순서 ON", the text is :{text}\n=======')
                        continue

                    if text in sentence:
                        print(f'text already exist, text is {text} | sentence is {sentence}\n=======')
                        boolFound = False
                        continue

                    boolFound = False
                    text = text.replace("\n"," ")
                    print(f'removed every trash data at text, the text is :{text}\n=======')

                    sentence += text

                if(len(sentence) < minLen):
                    print("article text not added! (len < minLen) | link : ",link)
                    print("the text is :", sentence)
                    print("============END======================\n\n\n")
                    continue


                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")
                print(sentence)
                text_list.append(sentence)
                print("============END======================\n\n\n")

            return text_list
        except Exception as e:
            
            print("error happend\n", e)

[2022-10-25 13:02:57] mrx-link.MRXLinkMagics.mrxlink_set_parameters() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='[]\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702868.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702868.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev', no_reply=True)
[2022-10-25 13:02:57] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='{"nodes":[{"id":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","name":"import_packages","code":"from selenium import webdriver\\nfrom selenium.webdriver.chrome.options import Options\\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\\nimport time\\nimport re\\nimport dill as pickle\\nimport datetime\\nimport requests\\nimport json\\nimport tqdm","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"0d487371-c4fc-44ed-a77e-87b5b293556f","name":"define_driver","code":"def get_selenium_driver(ip):\\n    capabilities = {\\n        \\"browserName\\": \\"chrome\\",\\n        \\"browserVersion\\": \\"latest\\",\\n        \\"selenoid:options\\": {\\n            \\"enableVNC\\": True,\\n            \\"enableVideo\\": False\\n        }\\n    }\\n\\n    options = Options()\\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\\n    options.add_experimental_option(\\"excludeSwitches\\", [\\"enable-automation\\"])\\n    options.add_experimental_option(\'useAutomationExtension\', False)\\n    #options.add_argument(\\"--proxy-server=socks5://10.26.0.189:9050\\")\\n    options.add_argument(\\"--disable-blink-features=AutomationControlled\\")\\n\\n    driver = webdriver.Remote(\\n        command_executor=f\\"http://{ip}/callisto\\",\\n        options=options,\\n        desired_capabilities=capabilities)\\n\\n    return driver","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","name":"define_pickle","code":"def save_pickle(data, path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'wb\') as f:\\n        pickle.dump(data, f)\\n\\ndef load_pickle(path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'rb\') as f:\\n        data = pickle.load(f)\\n\\n    return data","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"b152beca-de2a-42ba-a62c-b4e34f932f1c","name":"define_crawl_class","code":"class crawl:\\n    def __init__(self, galleryId):\\n        self.galleryId = galleryId\\n        \\n    def allocate_driver(self, ip):\\n        self.driver = get_selenium_driver(ip)\\n        \\n    def delocalte_driver(self):\\n        driver.close()\\n        driver.quit()\\n    \\n    def articleList(self, page):\\n        id = self.galleryId\\n        url = f\\"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend\\"\\n        self.driver.get(url)\\n        self.driver.implicitly_wait(3)\\n        time.sleep(1)\\n        \\n        article_list = self.driver.find_element(By.TAG_NAME, \\"tbody\\").find_elements(By.TAG_NAME, \\"tr\\")\\n        return article_list\\n    \\n    def articleLink(self, article_list):\\n        title_link = []\\n        \\n        try:\\n            for item in article_list:\\n\\n                title_item = item.find_element(By.TAG_NAME, \\"a\\")\\n                title_item = title_item.get_attribute(\'href\')\\n                if(title_item is None or self.galleryId not in title_item):\\n                    continue\\n\\n                title_link.append(title_item)\\n                print(\\"new title link added at title_link[\\",len(title_link)-1, \\"]:\\", title_item)\\n        \\n        except:\\n            return title_link\\n    \\n        return title_link\\n    \\n    \\n    \\n    def articleText(self, crawledLink, minLen):\\n        text_list = []\\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n        \\n        \\n        for link in crawledLink:\\n            print(\\"==============START====================\\")\\n            self.driver.get(link)\\n            self.driver.implicitly_wait(10)\\n            time.sleep(2)\\n            sentence = \\"\\"\\n            \\n            article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n            try:\\n                article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n                \\n            except:\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                except:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n            \\n    \\n            \\n            boolFound = False\\n            \\n            for i in article_text:\\n                text = i.text\\n                \\n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                if len(urls) > 0:\\n                    print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                    for str_link in urls:\\n                        text = text.replace(str_link, \\"\\")\\n                    boolFound = False\\n                    \\n                if \\"이미지 순서 ON\\" in text:\\n                    print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = True\\n                    continue\\n                \\n                if \\"- dc official App\\" in text:\\n                    print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                    \\n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                    print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                    continue\\n                \\n                if text in sentence:\\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                \\n                boolFound = False\\n                text = text.replace(\\"\\\\n\\",\\" \\")\\n                print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n                    \\n                sentence += text\\n\\n            if(len(sentence) < minLen):\\n                print(\\"article text not added! (len < minLen) | link : \\",link)\\n                print(\\"the text is :\\", sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                continue\\n\\n\\n            print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n            print(sentence)\\n            text_list.append(sentence)\\n            print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n            \\n        return text_list\\n    \\n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\\n        try:\\n            text_list = []\\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n\\n\\n            for link in tqdm.tqdm(crawledLink):\\n                print(\\"==============START====================\\")\\n                self.driver.get(link)\\n                self.driver.implicitly_wait(10)\\n                time.sleep(2)\\n\\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, \\"gall_date\\").text.split(\\" \\")\\n                articleDate = articleDateHour[0].split(\\".\\")\\n                articleHour = articleDateHour[1].split(\\":\\")\\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\\n\\n\\n                if(limUnixDate > articleUnixDate):\\n                    continue\\n\\n                sentence = \\"\\"\\n\\n                article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n\\n                except:\\n                    try:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                    except:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n\\n\\n\\n                boolFound = False\\n\\n                for i in article_text:\\n                    text = i.text\\n\\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                    if len(urls) > 0:\\n                        print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                        for str_link in urls:\\n                            text = text.replace(str_link, \\"\\")\\n                        boolFound = False\\n\\n                    if \\"이미지 순서 ON\\" in text:\\n                        print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = True\\n                        continue\\n\\n                    if \\"- dc official App\\" in text:\\n                        print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                        print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                        continue\\n\\n                    if text in sentence:\\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    boolFound = False\\n                    text = text.replace(\\"\\\\n\\",\\" \\")\\n                    print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n\\n                    sentence += text\\n\\n                if(len(sentence) < minLen):\\n                    print(\\"article text not added! (len < minLen) | link : \\",link)\\n                    print(\\"the text is :\\", sentence)\\n                    print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                    continue\\n\\n\\n                print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n                print(sentence)\\n                text_list.append(sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n\\n            return text_list\\n        except Exception as e:\\n            \\n            print(\\"error happend\\\\n\\", e)","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}}],"edges":[{"parent":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","child":"0d487371-c4fc-44ed-a77e-87b5b293556f"},{"parent":"0d487371-c4fc-44ed-a77e-87b5b293556f","child":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55"},{"parent":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","child":"b152beca-de2a-42ba-a62c-b4e34f932f1c"}]}\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702868.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702868.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev')
[2022-10-25 13:02:57] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'name': 'import_packages', 'code': 'from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'name': 'define_driver', 'code': 'def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'name': 'define_pickle', 'code': 'def save_pickle(data, path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'wb\') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'rb\') as f:\n        data = pickle.load(f)\n\n    return data', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'b152beca-de2a-42ba-a62c-b4e34f932f1c', 'name': 'define_crawl_class', 'code': 'class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in tqdm.tqdm(crawledLink):\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}], 'edges': [{'parent': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'child': '0d487371-c4fc-44ed-a77e-87b5b293556f'}, {'parent': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'child': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55'}, {'parent': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'child': 'b152beca-de2a-42ba-a62c-b4e34f932f1c'}]}
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:58] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='{"nodes":[{"id":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","name":"import_packages","code":"from selenium import webdriver\\nfrom selenium.webdriver.chrome.options import Options\\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\\nimport time\\nimport re\\nimport dill as pickle\\nimport datetime\\nimport requests\\nimport json\\nimport tqdm","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"0d487371-c4fc-44ed-a77e-87b5b293556f","name":"define_driver","code":"def get_selenium_driver(ip):\\n    capabilities = {\\n        \\"browserName\\": \\"chrome\\",\\n        \\"browserVersion\\": \\"latest\\",\\n        \\"selenoid:options\\": {\\n            \\"enableVNC\\": True,\\n            \\"enableVideo\\": False\\n        }\\n    }\\n\\n    options = Options()\\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\\n    options.add_experimental_option(\\"excludeSwitches\\", [\\"enable-automation\\"])\\n    options.add_experimental_option(\'useAutomationExtension\', False)\\n    #options.add_argument(\\"--proxy-server=socks5://10.26.0.189:9050\\")\\n    options.add_argument(\\"--disable-blink-features=AutomationControlled\\")\\n\\n    driver = webdriver.Remote(\\n        command_executor=f\\"http://{ip}/callisto\\",\\n        options=options,\\n        desired_capabilities=capabilities)\\n\\n    return driver","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","name":"define_pickle","code":"def save_pickle(data, path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'wb\') as f:\\n        pickle.dump(data, f)\\n\\ndef load_pickle(path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'rb\') as f:\\n        data = pickle.load(f)\\n\\n    return data","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"b152beca-de2a-42ba-a62c-b4e34f932f1c","name":"define_crawl_class","code":"class crawl:\\n    def __init__(self, galleryId):\\n        self.galleryId = galleryId\\n        \\n    def allocate_driver(self, ip):\\n        self.driver = get_selenium_driver(ip)\\n        \\n    def delocalte_driver(self):\\n        driver.close()\\n        driver.quit()\\n    \\n    def articleList(self, page):\\n        id = self.galleryId\\n        url = f\\"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend\\"\\n        self.driver.get(url)\\n        self.driver.implicitly_wait(3)\\n        time.sleep(1)\\n        \\n        article_list = self.driver.find_element(By.TAG_NAME, \\"tbody\\").find_elements(By.TAG_NAME, \\"tr\\")\\n        return article_list\\n    \\n    def articleLink(self, article_list):\\n        title_link = []\\n        \\n        try:\\n            for item in article_list:\\n\\n                title_item = item.find_element(By.TAG_NAME, \\"a\\")\\n                title_item = title_item.get_attribute(\'href\')\\n                if(title_item is None or self.galleryId not in title_item):\\n                    continue\\n\\n                title_link.append(title_item)\\n                print(\\"new title link added at title_link[\\",len(title_link)-1, \\"]:\\", title_item)\\n        \\n        except:\\n            return title_link\\n    \\n        return title_link\\n    \\n    \\n    \\n    def articleText(self, crawledLink, minLen):\\n        text_list = []\\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n        \\n        \\n        for link in crawledLink:\\n            print(\\"==============START====================\\")\\n            self.driver.get(link)\\n            self.driver.implicitly_wait(10)\\n            time.sleep(2)\\n            sentence = \\"\\"\\n            \\n            article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n            try:\\n                article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n                \\n            except:\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                except:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n            \\n    \\n            \\n            boolFound = False\\n            \\n            for i in article_text:\\n                text = i.text\\n                \\n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                if len(urls) > 0:\\n                    print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                    for str_link in urls:\\n                        text = text.replace(str_link, \\"\\")\\n                    boolFound = False\\n                    \\n                if \\"이미지 순서 ON\\" in text:\\n                    print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = True\\n                    continue\\n                \\n                if \\"- dc official App\\" in text:\\n                    print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                    \\n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                    print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                    continue\\n                \\n                if text in sentence:\\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                \\n                boolFound = False\\n                text = text.replace(\\"\\\\n\\",\\" \\")\\n                print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n                    \\n                sentence += text\\n\\n            if(len(sentence) < minLen):\\n                print(\\"article text not added! (len < minLen) | link : \\",link)\\n                print(\\"the text is :\\", sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                continue\\n\\n\\n            print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n            print(sentence)\\n            text_list.append(sentence)\\n            print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n            \\n        return text_list\\n    \\n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\\n        try:\\n            text_list = []\\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n\\n\\n            for link in tqdm.tqdm(crawledLink):\\n                print(\\"==============START====================\\")\\n                self.driver.get(link)\\n                self.driver.implicitly_wait(10)\\n                time.sleep(2)\\n\\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, \\"gall_date\\").text.split(\\" \\")\\n                articleDate = articleDateHour[0].split(\\".\\")\\n                articleHour = articleDateHour[1].split(\\":\\")\\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\\n\\n\\n                if(limUnixDate > articleUnixDate):\\n                    continue\\n\\n                sentence = \\"\\"\\n\\n                article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n\\n                except:\\n                    try:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                    except:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n\\n\\n\\n                boolFound = False\\n\\n                for i in article_text:\\n                    text = i.text\\n\\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                    if len(urls) > 0:\\n                        print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                        for str_link in urls:\\n                            text = text.replace(str_link, \\"\\")\\n                        boolFound = False\\n\\n                    if \\"이미지 순서 ON\\" in text:\\n                        print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = True\\n                        continue\\n\\n                    if \\"- dc official App\\" in text:\\n                        print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                        print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                        continue\\n\\n                    if text in sentence:\\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    boolFound = False\\n                    text = text.replace(\\"\\\\n\\",\\" \\")\\n                    print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n\\n                    sentence += text\\n\\n                if(len(sentence) < minLen):\\n                    print(\\"article text not added! (len < minLen) | link : \\",link)\\n                    print(\\"the text is :\\", sentence)\\n                    print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                    continue\\n\\n\\n                print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n                print(sentence)\\n                text_list.append(sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n\\n            return text_list\\n        except Exception as e:\\n            \\n            print(\\"error happend\\\\n\\", e)","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}}],"edges":[{"parent":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","child":"0d487371-c4fc-44ed-a77e-87b5b293556f"},{"parent":"0d487371-c4fc-44ed-a77e-87b5b293556f","child":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55"},{"parent":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","child":"b152beca-de2a-42ba-a62c-b4e34f932f1c"}]}\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702978.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702978.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev')
[2022-10-25 13:02:59] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'name': 'import_packages', 'code': 'from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'name': 'define_driver', 'code': 'def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'name': 'define_pickle', 'code': 'def save_pickle(data, path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'wb\') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'rb\') as f:\n        data = pickle.load(f)\n\n    return data', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'b152beca-de2a-42ba-a62c-b4e34f932f1c', 'name': 'define_crawl_class', 'code': 'class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in tqdm.tqdm(crawledLink):\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}], 'edges': [{'parent': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'child': '0d487371-c4fc-44ed-a77e-87b5b293556f'}, {'parent': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'child': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55'}, {'parent': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'child': 'b152beca-de2a-42ba-a62c-b4e34f932f1c'}]}
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:02:59] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='{"nodes":[{"id":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","name":"import_packages","code":"from selenium import webdriver\\nfrom selenium.webdriver.chrome.options import Options\\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\\nimport time\\nimport re\\nimport dill as pickle\\nimport datetime\\nimport requests\\nimport json\\nimport tqdm","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"0d487371-c4fc-44ed-a77e-87b5b293556f","name":"define_driver","code":"def get_selenium_driver(ip):\\n    capabilities = {\\n        \\"browserName\\": \\"chrome\\",\\n        \\"browserVersion\\": \\"latest\\",\\n        \\"selenoid:options\\": {\\n            \\"enableVNC\\": True,\\n            \\"enableVideo\\": False\\n        }\\n    }\\n\\n    options = Options()\\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\\n    options.add_experimental_option(\\"excludeSwitches\\", [\\"enable-automation\\"])\\n    options.add_experimental_option(\'useAutomationExtension\', False)\\n    #options.add_argument(\\"--proxy-server=socks5://10.26.0.189:9050\\")\\n    options.add_argument(\\"--disable-blink-features=AutomationControlled\\")\\n\\n    driver = webdriver.Remote(\\n        command_executor=f\\"http://{ip}/callisto\\",\\n        options=options,\\n        desired_capabilities=capabilities)\\n\\n    return driver","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","name":"define_pickle","code":"def save_pickle(data, path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'wb\') as f:\\n        pickle.dump(data, f)\\n\\ndef load_pickle(path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'rb\') as f:\\n        data = pickle.load(f)\\n\\n    return data","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"b152beca-de2a-42ba-a62c-b4e34f932f1c","name":"define_crawl_class","code":"class crawl:\\n    def __init__(self, galleryId):\\n        self.galleryId = galleryId\\n        \\n    def allocate_driver(self, ip):\\n        self.driver = get_selenium_driver(ip)\\n        \\n    def delocalte_driver(self):\\n        driver.close()\\n        driver.quit()\\n    \\n    def articleList(self, page):\\n        id = self.galleryId\\n        url = f\\"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend\\"\\n        self.driver.get(url)\\n        self.driver.implicitly_wait(3)\\n        time.sleep(1)\\n        \\n        article_list = self.driver.find_element(By.TAG_NAME, \\"tbody\\").find_elements(By.TAG_NAME, \\"tr\\")\\n        return article_list\\n    \\n    def articleLink(self, article_list):\\n        title_link = []\\n        \\n        try:\\n            for item in article_list:\\n\\n                title_item = item.find_element(By.TAG_NAME, \\"a\\")\\n                title_item = title_item.get_attribute(\'href\')\\n                if(title_item is None or self.galleryId not in title_item):\\n                    continue\\n\\n                title_link.append(title_item)\\n                print(\\"new title link added at title_link[\\",len(title_link)-1, \\"]:\\", title_item)\\n        \\n        except:\\n            return title_link\\n    \\n        return title_link\\n    \\n    \\n    \\n    def articleText(self, crawledLink, minLen):\\n        text_list = []\\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n        \\n        \\n        for link in crawledLink:\\n            print(\\"==============START====================\\")\\n            self.driver.get(link)\\n            self.driver.implicitly_wait(10)\\n            time.sleep(2)\\n            sentence = \\"\\"\\n            \\n            article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n            try:\\n                article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n                \\n            except:\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                except:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n            \\n    \\n            \\n            boolFound = False\\n            \\n            for i in article_text:\\n                text = i.text\\n                \\n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                if len(urls) > 0:\\n                    print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                    for str_link in urls:\\n                        text = text.replace(str_link, \\"\\")\\n                    boolFound = False\\n                    \\n                if \\"이미지 순서 ON\\" in text:\\n                    print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = True\\n                    continue\\n                \\n                if \\"- dc official App\\" in text:\\n                    print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                    \\n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                    print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                    continue\\n                \\n                if text in sentence:\\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                \\n                boolFound = False\\n                text = text.replace(\\"\\\\n\\",\\" \\")\\n                print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n                    \\n                sentence += text\\n\\n            if(len(sentence) < minLen):\\n                print(\\"article text not added! (len < minLen) | link : \\",link)\\n                print(\\"the text is :\\", sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                continue\\n\\n\\n            print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n            print(sentence)\\n            text_list.append(sentence)\\n            print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n            \\n        return text_list\\n    \\n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\\n        try:\\n            text_list = []\\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n\\n\\n            for link in tqdm.tqdm(crawledLink):\\n                print(\\"==============START====================\\")\\n                self.driver.get(link)\\n                self.driver.implicitly_wait(10)\\n                time.sleep(2)\\n\\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, \\"gall_date\\").text.split(\\" \\")\\n                articleDate = articleDateHour[0].split(\\".\\")\\n                articleHour = articleDateHour[1].split(\\":\\")\\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\\n\\n\\n                if(limUnixDate > articleUnixDate):\\n                    continue\\n\\n                sentence = \\"\\"\\n\\n                article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n\\n                except:\\n                    try:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                    except:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n\\n\\n\\n                boolFound = False\\n\\n                for i in article_text:\\n                    text = i.text\\n\\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                    if len(urls) > 0:\\n                        print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                        for str_link in urls:\\n                            text = text.replace(str_link, \\"\\")\\n                        boolFound = False\\n\\n                    if \\"이미지 순서 ON\\" in text:\\n                        print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = True\\n                        continue\\n\\n                    if \\"- dc official App\\" in text:\\n                        print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                        print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                        continue\\n\\n                    if text in sentence:\\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    boolFound = False\\n                    text = text.replace(\\"\\\\n\\",\\" \\")\\n                    print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n\\n                    sentence += text\\n\\n                if(len(sentence) < minLen):\\n                    print(\\"article text not added! (len < minLen) | link : \\",link)\\n                    print(\\"the text is :\\", sentence)\\n                    print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                    continue\\n\\n\\n                print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n                print(sentence)\\n                text_list.append(sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n\\n            return text_list\\n        except Exception as e:\\n            \\n            print(\\"error happend\\\\n\\", e)","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}}],"edges":[{"parent":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","child":"0d487371-c4fc-44ed-a77e-87b5b293556f"},{"parent":"0d487371-c4fc-44ed-a77e-87b5b293556f","child":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55"},{"parent":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","child":"b152beca-de2a-42ba-a62c-b4e34f932f1c"}]}\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702979.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702979.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev')
[2022-10-25 13:03:00] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'name': 'import_packages', 'code': 'from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'name': 'define_driver', 'code': 'def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'name': 'define_pickle', 'code': 'def save_pickle(data, path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'wb\') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'rb\') as f:\n        data = pickle.load(f)\n\n    return data', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'b152beca-de2a-42ba-a62c-b4e34f932f1c', 'name': 'define_crawl_class', 'code': 'class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in tqdm.tqdm(crawledLink):\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}], 'edges': [{'parent': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'child': '0d487371-c4fc-44ed-a77e-87b5b293556f'}, {'parent': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'child': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55'}, {'parent': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'child': 'b152beca-de2a-42ba-a62c-b4e34f932f1c'}]}
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:00] mrx-link.MRXLinkDag.mrxlink_update_dag() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:02] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm\n', id='455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', name='Unknown', type='CodeCell')
[2022-10-25 13:03:02] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:02] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:02] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:02] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:02] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:03] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:04] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf): {'Options': "<class 'selen...ions.Options'>", 'By': "<class 'selen...common.by.By'>", 'datetime': "<module 'date.../datetime.py'>", 'json': "<module 'json.../__init__.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'pickle': "<module 'dill.../__init__.py'>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>"}
[2022-10-25 13:03:04] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver\n', id='0d487371-c4fc-44ed-a77e-87b5b293556f', name='Unknown', type='CodeCell')
[2022-10-25 13:03:04] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:04] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:04] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:04] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:04] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:04] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f): {'By': "<class 'selen...common.by.By'>", 'get_selenium_driver': '<function run...x7f99deba44d0>', 'Options': "<class 'selen...ions.Options'>", 'datetime': "<module 'date.../datetime.py'>", 'json': "<module 'json.../__init__.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'pickle': "<module 'dill.../__init__.py'>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>"}
[2022-10-25 13:03:05] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='def save_pickle(data, path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'wb\') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'rb\') as f:\n        data = pickle.load(f)\n\n    return data\n', id='e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', name='Unknown', type='CodeCell')
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55): {'By': "<class 'selen...common.by.By'>", 'get_selenium_driver': '<function run...x7f99debb7440>', 'Options': "<class 'selen...ions.Options'>", 'save_pickle': '<function run...x7f99debb7ef0>', 'load_pickle': '<function run...x7f99debb7320>', 'datetime': "<module 'date.../datetime.py'>", 'json': "<module 'json.../__init__.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'requests': "<module 'requ.../__init__.py'>", 'tqdm': "<module 'tqdm.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>", 'pickle': "<module 'dill.../__init__.py'>"}
[2022-10-25 13:03:05] mrx-link.MRXLinkMagics.mrxlink_execute_node() DEBUG: args: Namespace(cell='class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in tqdm.tqdm(crawledLink):\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)\n', id='b152beca-de2a-42ba-a62c-b4e34f932f1c', name='Unknown', type='CodeCell')
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.INVALID -> MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkDag.mrxlink_execute_node() DEBUG: Clear import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf), status MRXLinkComponentStatus.INVALID
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component import_packages (455bc4ff-d374-4a11-b6f8-cdd8b532e8bf) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_driver (0d487371-c4fc-44ed-a77e-87b5b293556f) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_pickle (e9fd5d25-e0f6-46d1-8165-eafc49fc0c55) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c) status MRXLinkComponentStatus.RUNNING -> MRXLinkComponentStatus.RUNNING
[2022-10-25 13:03:05] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: Component define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c) status MRXLinkComponentStatus.SUCCESSFUL -> MRXLinkComponentStatus.SUCCESSFUL
[2022-10-25 13:03:06] mrx-link.MRXLinkComponentCodeCell.mrxlink_execute_node() DEBUG: define_crawl_class (b152beca-de2a-42ba-a62c-b4e34f932f1c): {'Options': "<class 'selen...ions.Options'>", 'save_pickle': '<function run...x7f99deba2c20>', 'load_pickle': '<function run...x7f99deba2b90>', 'crawl': "<class 'execu...ocals>.crawl'>", 'By': "<class 'selen...common.by.By'>", 'get_selenium_driver': '<function run...x7f99deba2170>', 'json': "<module 'json.../__init__.py'>", 'requests': "<module 'requ.../__init__.py'>", 'webdriver': "<module 'sele.../__init__.py'>", 'pickle': "<module 'dill.../__init__.py'>", 'datetime': "<module 'date.../datetime.py'>", 're': "<module 're' ...hon3.7/re.py'>", 'time': "<module 'time' (built-in)>", 'tqdm': "<module 'tqdm.../__init__.py'>"}
[2022-10-25 13:03:24] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: args: Namespace(base_url='http://jupyter-alsruf36--tend-2danalyzer:8888/user/alsruf36/tend-analyzer/', cell='{"nodes":[{"id":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","name":"import_packages","code":"from selenium import webdriver\\nfrom selenium.webdriver.chrome.options import Options\\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\\nimport time\\nimport re\\nimport dill as pickle\\nimport datetime\\nimport requests\\nimport json\\nimport tqdm","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"0d487371-c4fc-44ed-a77e-87b5b293556f","name":"define_driver","code":"def get_selenium_driver(ip):\\n    capabilities = {\\n        \\"browserName\\": \\"chrome\\",\\n        \\"browserVersion\\": \\"latest\\",\\n        \\"selenoid:options\\": {\\n            \\"enableVNC\\": True,\\n            \\"enableVideo\\": False\\n        }\\n    }\\n\\n    options = Options()\\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\\n    options.add_experimental_option(\\"excludeSwitches\\", [\\"enable-automation\\"])\\n    options.add_experimental_option(\'useAutomationExtension\', False)\\n    #options.add_argument(\\"--proxy-server=socks5://10.26.0.189:9050\\")\\n    options.add_argument(\\"--disable-blink-features=AutomationControlled\\")\\n\\n    driver = webdriver.Remote(\\n        command_executor=f\\"http://{ip}/callisto\\",\\n        options=options,\\n        desired_capabilities=capabilities)\\n\\n    return driver","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","name":"define_pickle","code":"def save_pickle(data, path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'wb\') as f:\\n        pickle.dump(data, f)\\n\\ndef load_pickle(path, name):\\n    pickle_path = f\\"{path}/{name}.pkl\\"\\n\\n    with open(pickle_path, \'rb\') as f:\\n        data = pickle.load(f)\\n\\n    return data","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}},{"id":"b152beca-de2a-42ba-a62c-b4e34f932f1c","name":"define_crawl_class","code":"class crawl:\\n    def __init__(self, galleryId):\\n        self.galleryId = galleryId\\n        \\n    def allocate_driver(self, ip):\\n        self.driver = get_selenium_driver(ip)\\n        \\n    def delocalte_driver(self):\\n        driver.close()\\n        driver.quit()\\n    \\n    def articleList(self, page):\\n        id = self.galleryId\\n        url = f\\"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend\\"\\n        self.driver.get(url)\\n        self.driver.implicitly_wait(3)\\n        time.sleep(1)\\n        \\n        article_list = self.driver.find_element(By.TAG_NAME, \\"tbody\\").find_elements(By.TAG_NAME, \\"tr\\")\\n        return article_list\\n    \\n    def articleLink(self, article_list):\\n        title_link = []\\n        \\n        try:\\n            for item in article_list:\\n\\n                title_item = item.find_element(By.TAG_NAME, \\"a\\")\\n                title_item = title_item.get_attribute(\'href\')\\n                if(title_item is None or self.galleryId not in title_item):\\n                    continue\\n\\n                title_link.append(title_item)\\n                print(\\"new title link added at title_link[\\",len(title_link)-1, \\"]:\\", title_item)\\n        \\n        except:\\n            return title_link\\n    \\n        return title_link\\n    \\n    \\n    \\n    def articleText(self, crawledLink, minLen):\\n        text_list = []\\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n        \\n        \\n        for link in crawledLink:\\n            print(\\"==============START====================\\")\\n            self.driver.get(link)\\n            self.driver.implicitly_wait(10)\\n            time.sleep(2)\\n            sentence = \\"\\"\\n            \\n            article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n            try:\\n                article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n                \\n            except:\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                except:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n            \\n    \\n            \\n            boolFound = False\\n            \\n            for i in article_text:\\n                text = i.text\\n                \\n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                if len(urls) > 0:\\n                    print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                    for str_link in urls:\\n                        text = text.replace(str_link, \\"\\")\\n                    boolFound = False\\n                    \\n                if \\"이미지 순서 ON\\" in text:\\n                    print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = True\\n                    continue\\n                \\n                if \\"- dc official App\\" in text:\\n                    print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                    \\n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                    print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                    continue\\n                \\n                if text in sentence:\\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                    boolFound = False\\n                    continue\\n                \\n                boolFound = False\\n                text = text.replace(\\"\\\\n\\",\\" \\")\\n                print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n                    \\n                sentence += text\\n\\n            if(len(sentence) < minLen):\\n                print(\\"article text not added! (len < minLen) | link : \\",link)\\n                print(\\"the text is :\\", sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                continue\\n\\n\\n            print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n            print(sentence)\\n            text_list.append(sentence)\\n            print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n            \\n        return text_list\\n    \\n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\\n        try:\\n            text_list = []\\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\\n\\n\\n            for link in tqdm.tqdm(crawledLink):\\n                print(\\"==============START====================\\")\\n                self.driver.get(link)\\n                self.driver.implicitly_wait(10)\\n                time.sleep(2)\\n\\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, \\"gall_date\\").text.split(\\" \\")\\n                articleDate = articleDateHour[0].split(\\".\\")\\n                articleHour = articleDateHour[1].split(\\":\\")\\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\\n\\n\\n                if(limUnixDate > articleUnixDate):\\n                    continue\\n\\n                sentence = \\"\\"\\n\\n                article_text = self.driver.find_element(By.CLASS_NAME, \\"write_div\\")\\n                try:\\n                    article_text = article_text.find_elements(By.TAG_NAME, \\"p\\")\\n\\n                except:\\n                    try:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"div\\")\\n                    except:\\n                        article_text = article_text.find_elements(By.TAG_NAME, \\"span\\")\\n\\n\\n\\n                boolFound = False\\n\\n                for i in article_text:\\n                    text = i.text\\n\\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\\n                    if len(urls) > 0:\\n                        print(f\'url exist, replaced. the text is :{text}\\\\n=======\')\\n                        for str_link in urls:\\n                            text = text.replace(str_link, \\"\\")\\n                        boolFound = False\\n\\n                    if \\"이미지 순서 ON\\" in text:\\n                        print(f\'\\"이미지 순서 ON\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = True\\n                        continue\\n\\n                    if \\"- dc official App\\" in text:\\n                        print(f\'\\"- dc official App\\" exist, replaced. the text is :{text}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or \\" \\" in text):\\n                        print(f\'number after \\"이미지 순서 ON\\", the text is :{text}\\\\n=======\')\\n                        continue\\n\\n                    if text in sentence:\\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\\\n=======\')\\n                        boolFound = False\\n                        continue\\n\\n                    boolFound = False\\n                    text = text.replace(\\"\\\\n\\",\\" \\")\\n                    print(f\'removed every trash data at text, the text is :{text}\\\\n=======\')\\n\\n                    sentence += text\\n\\n                if(len(sentence) < minLen):\\n                    print(\\"article text not added! (len < minLen) | link : \\",link)\\n                    print(\\"the text is :\\", sentence)\\n                    print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n                    continue\\n\\n\\n                print(\\"article text added at text_list[\\",len(text_list),\\"]\\",\\"(link :\\",link,\\") :\\")\\n                print(sentence)\\n                text_list.append(sentence)\\n                print(\\"============END======================\\\\n\\\\n\\\\n\\")\\n\\n            return text_list\\n        except Exception as e:\\n            \\n            print(\\"error happend\\\\n\\", e)","metadata":{"componentType":"CodeCell","headerColor":"inherit","comments":[],"diskcache":false}}],"edges":[{"parent":"455bc4ff-d374-4a11-b6f8-cdd8b532e8bf","child":"0d487371-c4fc-44ed-a77e-87b5b293556f"},{"parent":"0d487371-c4fc-44ed-a77e-87b5b293556f","child":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55"},{"parent":"e9fd5d25-e0f6-46d1-8165-eafc49fc0c55","child":"b152beca-de2a-42ba-a62c-b4e34f932f1c"}]}\n', cookie='_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20_ga=GA1.1.1397462606.1660579579;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702983.0.0.0;%20_xsrf=2%7Cd0913eeb%7Cdea45f5b6b63989d6cff2b78cd602438%7C1666617898;%20jupyterhub-user-alsruf36-tend-analyzer=2%7C1:0%7C10:1666692752%7C38:jupyterhub-user-alsruf36-tend-analyzer%7C40:SGdwV1FaeFlQSTZCR2RBV2FOS2VDY1ZGWTFydmVY%7C2459007f5c2e06b675e273a7662f65081cd3093118a37706eb0be0d7975176bf;%20_ga=GA1.1.1397462606.1660579579;%20jupyterhub-session-id=821e12c8ffcb4d1bacd359e9f470fe0e;%20_ga_R3VN4GNEX2=GS1.1.1666701282.34.1.1666702983.0.0.0', header='Accept-Encoding=gzip,%20deflate,%20br;Host=jupyter.kshs.dev')
[2022-10-25 13:03:24] mrx-link.MRXLinkMagics.mrxlink_update_dag() DEBUG: graph: {'nodes': [{'id': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'name': 'import_packages', 'code': 'from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By #이거 나중에 옮겨야될거같긴한데 어따옮길지 민결이한테 물어보기\nimport time\nimport re\nimport dill as pickle\nimport datetime\nimport requests\nimport json\nimport tqdm', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'name': 'define_driver', 'code': 'def get_selenium_driver(ip):\n    capabilities = {\n        "browserName": "chrome",\n        "browserVersion": "latest",\n        "selenoid:options": {\n            "enableVNC": True,\n            "enableVideo": False\n        }\n    }\n\n    options = Options()\n    options.add_argument(\'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\')\n    options.add_experimental_option("excludeSwitches", ["enable-automation"])\n    options.add_experimental_option(\'useAutomationExtension\', False)\n    #options.add_argument("--proxy-server=socks5://10.26.0.189:9050")\n    options.add_argument("--disable-blink-features=AutomationControlled")\n\n    driver = webdriver.Remote(\n        command_executor=f"http://{ip}/callisto",\n        options=options,\n        desired_capabilities=capabilities)\n\n    return driver', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'name': 'define_pickle', 'code': 'def save_pickle(data, path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'wb\') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(path, name):\n    pickle_path = f"{path}/{name}.pkl"\n\n    with open(pickle_path, \'rb\') as f:\n        data = pickle.load(f)\n\n    return data', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}, {'id': 'b152beca-de2a-42ba-a62c-b4e34f932f1c', 'name': 'define_crawl_class', 'code': 'class crawl:\n    def __init__(self, galleryId):\n        self.galleryId = galleryId\n        \n    def allocate_driver(self, ip):\n        self.driver = get_selenium_driver(ip)\n        \n    def delocalte_driver(self):\n        driver.close()\n        driver.quit()\n    \n    def articleList(self, page):\n        id = self.galleryId\n        url = f"https://gall.dcinside.com/mgallery/board/lists?id={id}&page={page}&exception_mode=recommend"\n        self.driver.get(url)\n        self.driver.implicitly_wait(3)\n        time.sleep(1)\n        \n        article_list = self.driver.find_element(By.TAG_NAME, "tbody").find_elements(By.TAG_NAME, "tr")\n        return article_list\n    \n    def articleLink(self, article_list):\n        title_link = []\n        \n        try:\n            for item in article_list:\n\n                title_item = item.find_element(By.TAG_NAME, "a")\n                title_item = title_item.get_attribute(\'href\')\n                if(title_item is None or self.galleryId not in title_item):\n                    continue\n\n                title_link.append(title_item)\n                print("new title link added at title_link[",len(title_link)-1, "]:", title_item)\n        \n        except:\n            return title_link\n    \n        return title_link\n    \n    \n    \n    def articleText(self, crawledLink, minLen):\n        text_list = []\n        num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n        \n        \n        for link in crawledLink:\n            print("==============START====================")\n            self.driver.get(link)\n            self.driver.implicitly_wait(10)\n            time.sleep(2)\n            sentence = ""\n            \n            article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n            try:\n                article_text = article_text.find_elements(By.TAG_NAME, "p")\n                \n            except:\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "div")\n                except:\n                    article_text = article_text.find_elements(By.TAG_NAME, "span")\n            \n    \n            \n            boolFound = False\n            \n            for i in article_text:\n                text = i.text\n                \n                urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                if len(urls) > 0:\n                    print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                    for str_link in urls:\n                        text = text.replace(str_link, "")\n                    boolFound = False\n                    \n                if "이미지 순서 ON" in text:\n                    print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = True\n                    continue\n                \n                if "- dc official App" in text:\n                    print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                    boolFound = False\n                    continue\n                    \n                if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                    print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                    continue\n                \n                if text in sentence:\n                    print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                    boolFound = False\n                    continue\n                \n                boolFound = False\n                text = text.replace("\\n"," ")\n                print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n                    \n                sentence += text\n\n            if(len(sentence) < minLen):\n                print("article text not added! (len < minLen) | link : ",link)\n                print("the text is :", sentence)\n                print("============END======================\\n\\n\\n")\n                continue\n\n\n            print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n            print(sentence)\n            text_list.append(sentence)\n            print("============END======================\\n\\n\\n")\n            \n        return text_list\n    \n    def articleText_withLimit(self, crawledLink, minLen, limUnixDate):\n        try:\n            text_list = []\n            num = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n\n\n            for link in tqdm.tqdm(crawledLink):\n                print("==============START====================")\n                self.driver.get(link)\n                self.driver.implicitly_wait(10)\n                time.sleep(2)\n\n                articleDateHour = self.driver.find_element(By.CLASS_NAME, "gall_date").text.split(" ")\n                articleDate = articleDateHour[0].split(".")\n                articleHour = articleDateHour[1].split(":")\n                articleUnixDate = datetime.datetime(int(articleDate[0]), int(articleDate[1]), int(articleDate[2]), int(articleHour[0]), int(articleHour[0]),int(articleHour[0]), 0).timestamp()\n\n\n                if(limUnixDate > articleUnixDate):\n                    continue\n\n                sentence = ""\n\n                article_text = self.driver.find_element(By.CLASS_NAME, "write_div")\n                try:\n                    article_text = article_text.find_elements(By.TAG_NAME, "p")\n\n                except:\n                    try:\n                        article_text = article_text.find_elements(By.TAG_NAME, "div")\n                    except:\n                        article_text = article_text.find_elements(By.TAG_NAME, "span")\n\n\n\n                boolFound = False\n\n                for i in article_text:\n                    text = i.text\n\n                    urls = re.findall(\'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\', i.text)\n                    if len(urls) > 0:\n                        print(f\'url exist, replaced. the text is :{text}\\n=======\')\n                        for str_link in urls:\n                            text = text.replace(str_link, "")\n                        boolFound = False\n\n                    if "이미지 순서 ON" in text:\n                        print(f\'"이미지 순서 ON" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = True\n                        continue\n\n                    if "- dc official App" in text:\n                        print(f\'"- dc official App" exist, replaced. the text is :{text}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    if boolFound is True and ((text >= \'0\' and text <= \'9\') or " " in text):\n                        print(f\'number after "이미지 순서 ON", the text is :{text}\\n=======\')\n                        continue\n\n                    if text in sentence:\n                        print(f\'text already exist, text is {text} | sentence is {sentence}\\n=======\')\n                        boolFound = False\n                        continue\n\n                    boolFound = False\n                    text = text.replace("\\n"," ")\n                    print(f\'removed every trash data at text, the text is :{text}\\n=======\')\n\n                    sentence += text\n\n                if(len(sentence) < minLen):\n                    print("article text not added! (len < minLen) | link : ",link)\n                    print("the text is :", sentence)\n                    print("============END======================\\n\\n\\n")\n                    continue\n\n\n                print("article text added at text_list[",len(text_list),"]","(link :",link,") :")\n                print(sentence)\n                text_list.append(sentence)\n                print("============END======================\\n\\n\\n")\n\n            return text_list\n        except Exception as e:\n            \n            print("error happend\\n", e)', 'metadata': {'componentType': 'CodeCell', 'headerColor': 'inherit', 'comments': [], 'diskcache': False}}], 'edges': [{'parent': '455bc4ff-d374-4a11-b6f8-cdd8b532e8bf', 'child': '0d487371-c4fc-44ed-a77e-87b5b293556f'}, {'parent': '0d487371-c4fc-44ed-a77e-87b5b293556f', 'child': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55'}, {'parent': 'e9fd5d25-e0f6-46d1-8165-eafc49fc0c55', 'child': 'b152beca-de2a-42ba-a62c-b4e34f932f1c'}]}
